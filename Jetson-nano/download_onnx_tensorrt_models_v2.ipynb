{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eddc872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec\r\n",
      "=== Model Options ===\r\n",
      "  --uff=<file>                UFF model\r\n",
      "  --onnx=<file>               ONNX model\r\n",
      "  --model=<file>              Caffe model (default = no model, random weights used)\r\n",
      "  --deploy=<file>             Caffe prototxt file\r\n",
      "  --output=<name>[,<name>]*   Output names (it can be specified multiple times); at least one output is required for UFF and Caffe\r\n",
      "  --uffInput=<name>,X,Y,Z     Input blob name and its dimensions (X,Y,Z=C,H,W), it can be specified multiple times; at least one is required for UFF models\r\n",
      "  --uffNHWC                   Set if inputs are in the NHWC layout instead of NCHW (use X,Y,Z=H,W,C order in --uffInput)\r\n",
      "\r\n",
      "=== Build Options ===\r\n",
      "  --maxBatch                  Set max batch size and build an implicit batch engine (default = same size as --batch)\r\n",
      "                              This option should not be used when the input model is ONNX or when dynamic shapes are provided.\r\n",
      "  --minShapes=spec            Build with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapes=spec            Build with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapes=spec            Build with dynamic shapes using a profile with the max shapes provided\r\n",
      "  --minShapesCalib=spec       Calibrate with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapesCalib=spec       Calibrate with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapesCalib=spec       Calibrate with dynamic shapes using a profile with the max shapes provided\r\n",
      "                              Note: All three of min, opt and max shapes must be supplied.\r\n",
      "                                    However, if only opt shapes is supplied then it will be expanded so\r\n",
      "                                    that min shapes and max shapes are set to the same values as opt shapes.\r\n",
      "                                    Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --inputIOFormats=spec       Type and format of each of the input tensors (default = all inputs in fp32:chw)\r\n",
      "                              See --outputIOFormats help for the grammar of type and format list.\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    inputs following the same order as network inputs ID (even if only one input\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "  --outputIOFormats=spec      Type and format of each of the output tensors (default = all outputs in fp32:chw)\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    outputs following the same order as network outputs ID (even if only one output\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "                              IO Formats: spec  ::= IOfmt[\",\"spec]\r\n",
      "                                          IOfmt ::= type:fmt\r\n",
      "                                          type  ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"\r\n",
      "                                          fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\")[\"+\"fmt]\r\n",
      "  --workspace=N               Set workspace size in megabytes (default = 16)\r\n",
      "  --profilingVerbosity=mode   Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only)\r\n",
      "  --minTiming=M               Set the minimum number of iterations used in kernel selection (default = 1)\r\n",
      "  --avgTiming=M               Set the number of times averaged in each iteration for kernel selection (default = 8)\r\n",
      "  --refit                     Mark the engine as refittable. This will allow the inspection of refittable layers \r\n",
      "                              and weights within the engine.\r\n",
      "  --sparsity=spec             Control sparsity (default = disabled). \r\n",
      "                              Sparsity: spec ::= \"disable\", \"enable\", \"force\"\r\n",
      "                              Note: Description about each of these options is as below\r\n",
      "                                    disable = do not enable sparse tactics in the builder (this is the default)\r\n",
      "                                    enable  = enable sparse tactics in the builder (but these tactics will only be\r\n",
      "                                              considered if the weights have the right sparsity pattern)\r\n",
      "                                    force   = enable sparse tactics in the builder and force-overwrite the weights to have\r\n",
      "                                              a sparsity pattern (even if you loaded a model yourself)\r\n",
      "  --noTF32                    Disable tf32 precision (default is to enable tf32, in addition to fp32)\r\n",
      "  --fp16                      Enable fp16 precision, in addition to fp32 (default = disabled)\r\n",
      "  --int8                      Enable int8 precision, in addition to fp32 (default = disabled)\r\n",
      "  --best                      Enable all precisions to achieve the best performance (default = disabled)\r\n",
      "  --directIO                  Avoid reformatting at network boundaries. (default = disabled)\r\n",
      "  --precisionConstraints=spec Control precision constraints. (default = none)\r\n",
      "                                  Precision Constaints: spec ::= \"none\" | \"obey\" | \"prefer\"\r\n",
      "                                  none = no constraints\r\n",
      "                                  prefer = meet precision constraints if possible\r\n",
      "                                  obey = meet precision constraints or fail otherwise\r\n",
      "  --calib=<file>              Read INT8 calibration cache file\r\n",
      "  --safe                      Enable build safety certified engine\r\n",
      "  --consistency               Perform consistency checking on safety certified engine\r\n",
      "  --restricted                Enable safety scope checking with kSAFETY_SCOPE build flag\r\n",
      "  --saveEngine=<file>         Save the serialized engine\r\n",
      "  --loadEngine=<file>         Load a serialized engine\r\n",
      "  --tacticSources=tactics     Specify the tactics to be used by adding (+) or removing (-) tactics from the default \r\n",
      "                              tactic sources (default = all available tactics).\r\n",
      "                              Note: Currently only cuDNN, cuBLAS and cuBLAS-LT are listed as optional tactics.\r\n",
      "                              Tactic Sources: tactics ::= [\",\"tactic]\r\n",
      "                                              tactic  ::= (+|-)lib\r\n",
      "                                              lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"\r\n",
      "                              For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\r\n",
      "  --noBuilderCache            Disable timing cache in builder (default is to enable timing cache)\r\n",
      "  --timingCacheFile=<file>    Save/load the serialized global timing cache\r\n",
      "\r\n",
      "=== Inference Options ===\r\n",
      "  --batch=N                   Set batch size for implicit batch engines (default = 1)\r\n",
      "                              This option should not be used when the engine is built from an ONNX model or when dynamic\r\n",
      "                              shapes are provided when the engine is built.\r\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\r\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\r\n",
      "                              Input values spec ::= Ival[\",\"spec]\r\n",
      "                                           Ival ::= name\":\"file\r\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\r\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\r\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\r\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\r\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\r\n",
      "  --streams=N                 Instantiate N engines to use concurrently (default = 1)\r\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\r\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\r\n",
      "  --useManagedMemory          Use managed memory instead of seperate host and device allocations (default = disabled).\r\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\r\n",
      "  --threads                   Enable multithreading to drive engines with independent threads (default = disabled)\r\n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\r\n",
      "                              This flag may be ignored if the graph capture fails.\r\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\r\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\r\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\r\n",
      "  --buildOnly                 Skip inference perf measurement (default = disabled)\r\n",
      "\r\n",
      "=== Build and Inference Batch Options ===\r\n",
      "                              When using implicit batch, the max batch size of the engine, if not given, \r\n",
      "                              is set to the inference batch size;\r\n",
      "                              when using explicit batch, if shapes are specified only for inference, they \r\n",
      "                              will be used also as min/opt/max in the build profile; if shapes are \r\n",
      "                              specified only for the build, the opt shapes will be used also for inference;\r\n",
      "                              if both are specified, they must be compatible; and if explicit batch is \r\n",
      "                              enabled but neither is specified, the model must provide complete static\r\n",
      "                              dimensions, including batch size, for all inputs\r\n",
      "                              Using ONNX models automatically forces explicit batch.\r\n",
      "\r\n",
      "=== Reporting Options ===\r\n",
      "  --verbose                   Use verbose logging (default = false)\r\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\r\n",
      "  --percentile=P              Report performance for the P percentage (0<=P<=100, 0 representing max perf, and 100 representing min perf; (default = 99%)\r\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\r\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\r\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\r\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\r\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\r\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\r\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\r\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\r\n",
      "\r\n",
      "=== System Options ===\r\n",
      "  --device=N                  Select cuda device N (default = 0)\r\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\r\n",
      "  --allowGPUFallback          When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\r\n",
      "  --plugins                   Plugin library (.so) to load (can be specified multiple times)\r\n",
      "\r\n",
      "=== Help ===\r\n",
      "  --help, -h                  Print this message\r\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34e134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494fa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_MODEL_DIR = './Flask/Models/'\n",
    "\n",
    "FLASK_MODEL_DIR = './Flask/Models/'\n",
    "\n",
    "TRITON_MODEL_DIR = './Triton/Models/'\n",
    "TRITON_CONFIG_FILE = 'config.pbtxt'\n",
    "TRITON_MODEL_FILE = 'model.plan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3386000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensorrt_model(onnx_model, trt_model, precision, config):\n",
    "    trt_model = trt_model + '_' + precision\n",
    "    onnx_model_path = os.path.join(ONNX_MODEL_DIR, onnx_model + '.onnx')\n",
    "    flask_model_path = os.path.join(FLASK_MODEL_DIR, trt_model + '.plan')\n",
    "\n",
    "    !trtexec \\\n",
    "        --onnx=$onnx_model_path \\\n",
    "        --explicitBatch \\\n",
    "        --$precision \\\n",
    "        --saveEngine=$flask_model_path\n",
    "\n",
    "    triton_config_path = os.path.join(TRITON_MODEL_DIR, trt_model, TRITON_CONFIG_FILE)\n",
    "    os.makedirs(os.path.dirname(triton_config_path), exist_ok=True)\n",
    "    with open(triton_config_path, 'w') as f:\n",
    "        f.write(config.strip())\n",
    "\n",
    "    triton_model_path = os.path.join(TRITON_MODEL_DIR, trt_model, '1', TRITON_MODEL_FILE)\n",
    "    os.makedirs(os.path.dirname(triton_model_path), exist_ok=True)\n",
    "    !cp $flask_model_path $triton_model_path\n",
    "        \n",
    "    print(flask_model_path)\n",
    "    print(triton_config_path)\n",
    "    print(triton_model_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9df9d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: trtexec: command not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-64_best.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-64_best.plan\n",
      "./Triton/Models/resnet34-trt-64_best/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-64_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-64'\n",
    "trt_model = 'resnet34-trt-64'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 64, 64 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87cfdc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-64_fp16.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-64_fp16.plan\n",
      "./Triton/Models/resnet34-trt-64_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-64_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-64'\n",
    "trt_model = 'resnet34-trt-64'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 64, 64 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "443ffb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-128_fp16.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-128_fp16.plan\n",
      "./Triton/Models/resnet34-trt-128_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-128_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-128'\n",
    "trt_model = 'resnet34-trt-128'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 128, 128 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc3c391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-256_fp16.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-256_fp16.plan\n",
      "./Triton/Models/resnet34-trt-256_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-256_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-256'\n",
    "trt_model = 'resnet34-trt-256'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 256, 256 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2364d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-512_fp16.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-512_fp16.plan\n",
      "./Triton/Models/resnet34-trt-512_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-512_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-512'\n",
    "trt_model = 'resnet34-trt-512'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 512, 512 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a89e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt-1024_fp16.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt-1024_fp16.plan\n",
      "./Triton/Models/resnet34-trt-1024_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt-1024_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx-1024'\n",
    "trt_model = 'resnet34-trt-1024'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 1024, 1024 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b4c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa99ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/resnet34-trt_int8.plan': No such file or directory\n",
      "./Flask/Models/resnet34-trt_int8.plan\n",
      "./Triton/Models/resnet34-trt_int8/config.pbtxt\n",
      "./Triton/Models/resnet34-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx'\n",
    "trt_model = 'resnet34-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221adaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/mobilenet_v2-trt_best.plan': No such file or directory\n",
      "./Flask/Models/mobilenet_v2-trt_best.plan\n",
      "./Triton/Models/mobilenet_v2-trt_best/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84da281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/mobilenet_v2-trt_fp16.plan': No such file or directory\n",
      "./Flask/Models/mobilenet_v2-trt_fp16.plan\n",
      "./Triton/Models/mobilenet_v2-trt_fp16/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838b611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/mobilenet_v2-trt_int8.plan': No such file or directory\n",
      "./Flask/Models/mobilenet_v2-trt_int8.plan\n",
      "./Triton/Models/mobilenet_v2-trt_int8/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b2e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b0-trt_best.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b0-trt_best.plan\n",
      "./Triton/Models/efficientnet_b0-trt_best/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef3fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b0-trt_fp16.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b0-trt_fp16.plan\n",
      "./Triton/Models/efficientnet_b0-trt_fp16/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee83847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b0-trt_int8.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b0-trt_int8.plan\n",
      "./Triton/Models/efficientnet_b0-trt_int8/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b49a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b7-trt_best.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b7-trt_best.plan\n",
      "./Triton/Models/efficientnet_b7-trt_best/config.pbtxt\n",
      "./Triton/Models/efficientnet_b7-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b7-onnx'\n",
    "trt_model = 'efficientnet_b7-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 600, 600 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5af75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b7-trt_fp16.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b7-trt_fp16.plan\n",
      "./Triton/Models/efficientnet_b7-trt_fp16/config.pbtxt\n",
      "./Triton/Models/efficientnet_b7-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b7-onnx'\n",
    "trt_model = 'efficientnet_b7-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 600, 600 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b732a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: trtexec: not found\n",
      "cp: cannot stat './Flask/Models/efficientnet_b7-trt_int8.plan': No such file or directory\n",
      "./Flask/Models/efficientnet_b7-trt_int8.plan\n",
      "./Triton/Models/efficientnet_b7-trt_int8/config.pbtxt\n",
      "./Triton/Models/efficientnet_b7-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b7-onnx'\n",
    "trt_model = 'efficientnet_b7-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 600, 600 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
