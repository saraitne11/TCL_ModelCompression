{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eddc872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec\r\n",
      "=== Model Options ===\r\n",
      "  --uff=<file>                UFF model\r\n",
      "  --onnx=<file>               ONNX model\r\n",
      "  --model=<file>              Caffe model (default = no model, random weights used)\r\n",
      "  --deploy=<file>             Caffe prototxt file\r\n",
      "  --output=<name>[,<name>]*   Output names (it can be specified multiple times); at least one output is required for UFF and Caffe\r\n",
      "  --uffInput=<name>,X,Y,Z     Input blob name and its dimensions (X,Y,Z=C,H,W), it can be specified multiple times; at least one is required for UFF models\r\n",
      "  --uffNHWC                   Set if inputs are in the NHWC layout instead of NCHW (use X,Y,Z=H,W,C order in --uffInput)\r\n",
      "\r\n",
      "=== Build Options ===\r\n",
      "  --maxBatch                  Set max batch size and build an implicit batch engine (default = same size as --batch)\r\n",
      "                              This option should not be used when the input model is ONNX or when dynamic shapes are provided.\r\n",
      "  --minShapes=spec            Build with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapes=spec            Build with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapes=spec            Build with dynamic shapes using a profile with the max shapes provided\r\n",
      "  --minShapesCalib=spec       Calibrate with dynamic shapes using a profile with the min shapes provided\r\n",
      "  --optShapesCalib=spec       Calibrate with dynamic shapes using a profile with the opt shapes provided\r\n",
      "  --maxShapesCalib=spec       Calibrate with dynamic shapes using a profile with the max shapes provided\r\n",
      "                              Note: All three of min, opt and max shapes must be supplied.\r\n",
      "                                    However, if only opt shapes is supplied then it will be expanded so\r\n",
      "                                    that min shapes and max shapes are set to the same values as opt shapes.\r\n",
      "                                    Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256,input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --inputIOFormats=spec       Type and format of each of the input tensors (default = all inputs in fp32:chw)\r\n",
      "                              See --outputIOFormats help for the grammar of type and format list.\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    inputs following the same order as network inputs ID (even if only one input\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "  --outputIOFormats=spec      Type and format of each of the output tensors (default = all outputs in fp32:chw)\r\n",
      "                              Note: If this option is specified, please set comma-separated types and formats for all\r\n",
      "                                    outputs following the same order as network outputs ID (even if only one output\r\n",
      "                                    needs specifying IO format) or set the type and format once for broadcasting.\r\n",
      "                              IO Formats: spec  ::= IOfmt[\",\"spec]\r\n",
      "                                          IOfmt ::= type:fmt\r\n",
      "                                          type  ::= \"fp32\"|\"fp16\"|\"int32\"|\"int8\"\r\n",
      "                                          fmt   ::= (\"chw\"|\"chw2\"|\"chw4\"|\"hwc8\"|\"chw16\"|\"chw32\"|\"dhwc8\")[\"+\"fmt]\r\n",
      "  --workspace=N               Set workspace size in megabytes (default = 16)\r\n",
      "  --profilingVerbosity=mode   Specify profiling verbosity. mode ::= layer_names_only|detailed|none (default = layer_names_only)\r\n",
      "  --minTiming=M               Set the minimum number of iterations used in kernel selection (default = 1)\r\n",
      "  --avgTiming=M               Set the number of times averaged in each iteration for kernel selection (default = 8)\r\n",
      "  --refit                     Mark the engine as refittable. This will allow the inspection of refittable layers \r\n",
      "                              and weights within the engine.\r\n",
      "  --sparsity=spec             Control sparsity (default = disabled). \r\n",
      "                              Sparsity: spec ::= \"disable\", \"enable\", \"force\"\r\n",
      "                              Note: Description about each of these options is as below\r\n",
      "                                    disable = do not enable sparse tactics in the builder (this is the default)\r\n",
      "                                    enable  = enable sparse tactics in the builder (but these tactics will only be\r\n",
      "                                              considered if the weights have the right sparsity pattern)\r\n",
      "                                    force   = enable sparse tactics in the builder and force-overwrite the weights to have\r\n",
      "                                              a sparsity pattern (even if you loaded a model yourself)\r\n",
      "  --noTF32                    Disable tf32 precision (default is to enable tf32, in addition to fp32)\r\n",
      "  --fp16                      Enable fp16 precision, in addition to fp32 (default = disabled)\r\n",
      "  --int8                      Enable int8 precision, in addition to fp32 (default = disabled)\r\n",
      "  --best                      Enable all precisions to achieve the best performance (default = disabled)\r\n",
      "  --directIO                  Avoid reformatting at network boundaries. (default = disabled)\r\n",
      "  --precisionConstraints=spec Control precision constraints. (default = none)\r\n",
      "                                  Precision Constaints: spec ::= \"none\" | \"obey\" | \"prefer\"\r\n",
      "                                  none = no constraints\r\n",
      "                                  prefer = meet precision constraints if possible\r\n",
      "                                  obey = meet precision constraints or fail otherwise\r\n",
      "  --calib=<file>              Read INT8 calibration cache file\r\n",
      "  --safe                      Enable build safety certified engine\r\n",
      "  --consistency               Perform consistency checking on safety certified engine\r\n",
      "  --restricted                Enable safety scope checking with kSAFETY_SCOPE build flag\r\n",
      "  --saveEngine=<file>         Save the serialized engine\r\n",
      "  --loadEngine=<file>         Load a serialized engine\r\n",
      "  --tacticSources=tactics     Specify the tactics to be used by adding (+) or removing (-) tactics from the default \r\n",
      "                              tactic sources (default = all available tactics).\r\n",
      "                              Note: Currently only cuDNN, cuBLAS and cuBLAS-LT are listed as optional tactics.\r\n",
      "                              Tactic Sources: tactics ::= [\",\"tactic]\r\n",
      "                                              tactic  ::= (+|-)lib\r\n",
      "                                              lib     ::= \"CUBLAS\"|\"CUBLAS_LT\"|\"CUDNN\"\r\n",
      "                              For example, to disable cudnn and enable cublas: --tacticSources=-CUDNN,+CUBLAS\r\n",
      "  --noBuilderCache            Disable timing cache in builder (default is to enable timing cache)\r\n",
      "  --timingCacheFile=<file>    Save/load the serialized global timing cache\r\n",
      "\r\n",
      "=== Inference Options ===\r\n",
      "  --batch=N                   Set batch size for implicit batch engines (default = 1)\r\n",
      "                              This option should not be used when the engine is built from an ONNX model or when dynamic\r\n",
      "                              shapes are provided when the engine is built.\r\n",
      "  --shapes=spec               Set input shapes for dynamic shapes inference inputs.\r\n",
      "                              Note: Input names can be wrapped with escaped single quotes (ex: \\'Input:0\\').\r\n",
      "                              Example input shapes spec: input0:1x3x256x256, input1:1x3x128x128\r\n",
      "                              Each input shape is supplied as a key-value pair where key is the input name and\r\n",
      "                              value is the dimensions (including the batch dimension) to be used for that input.\r\n",
      "                              Each key-value pair has the key and value separated using a colon (:).\r\n",
      "                              Multiple input shapes can be provided via comma-separated key-value pairs.\r\n",
      "  --loadInputs=spec           Load input values from files (default = generate random inputs). Input names can be wrapped with single quotes (ex: 'Input:0')\r\n",
      "                              Input values spec ::= Ival[\",\"spec]\r\n",
      "                                           Ival ::= name\":\"file\r\n",
      "  --iterations=N              Run at least N inference iterations (default = 10)\r\n",
      "  --warmUp=N                  Run for N milliseconds to warmup before measuring performance (default = 200)\r\n",
      "  --duration=N                Run performance measurements for at least N seconds wallclock time (default = 3)\r\n",
      "  --sleepTime=N               Delay inference start with a gap of N milliseconds between launch and compute (default = 0)\r\n",
      "  --idleTime=N                Sleep N milliseconds between two continuous iterations(default = 0)\r\n",
      "  --streams=N                 Instantiate N engines to use concurrently (default = 1)\r\n",
      "  --exposeDMA                 Serialize DMA transfers to and from device (default = disabled).\r\n",
      "  --noDataTransfers           Disable DMA transfers to and from device (default = enabled).\r\n",
      "  --useManagedMemory          Use managed memory instead of seperate host and device allocations (default = disabled).\r\n",
      "  --useSpinWait               Actively synchronize on GPU events. This option may decrease synchronization time but increase CPU usage and power (default = disabled)\r\n",
      "  --threads                   Enable multithreading to drive engines with independent threads (default = disabled)\r\n",
      "  --useCudaGraph              Use CUDA graph to capture engine execution and then launch inference (default = disabled).\r\n",
      "                              This flag may be ignored if the graph capture fails.\r\n",
      "  --timeDeserialize           Time the amount of time it takes to deserialize the network and exit.\r\n",
      "  --timeRefit                 Time the amount of time it takes to refit the engine before inference.\r\n",
      "  --separateProfileRun        Do not attach the profiler in the benchmark run; if profiling is enabled, a second profile run will be executed (default = disabled)\r\n",
      "  --buildOnly                 Skip inference perf measurement (default = disabled)\r\n",
      "\r\n",
      "=== Build and Inference Batch Options ===\r\n",
      "                              When using implicit batch, the max batch size of the engine, if not given, \r\n",
      "                              is set to the inference batch size;\r\n",
      "                              when using explicit batch, if shapes are specified only for inference, they \r\n",
      "                              will be used also as min/opt/max in the build profile; if shapes are \r\n",
      "                              specified only for the build, the opt shapes will be used also for inference;\r\n",
      "                              if both are specified, they must be compatible; and if explicit batch is \r\n",
      "                              enabled but neither is specified, the model must provide complete static\r\n",
      "                              dimensions, including batch size, for all inputs\r\n",
      "                              Using ONNX models automatically forces explicit batch.\r\n",
      "\r\n",
      "=== Reporting Options ===\r\n",
      "  --verbose                   Use verbose logging (default = false)\r\n",
      "  --avgRuns=N                 Report performance measurements averaged over N consecutive iterations (default = 10)\r\n",
      "  --percentile=P              Report performance for the P percentage (0<=P<=100, 0 representing max perf, and 100 representing min perf; (default = 99%)\r\n",
      "  --dumpRefit                 Print the refittable layers and weights from a refittable engine\r\n",
      "  --dumpOutput                Print the output tensor(s) of the last inference iteration (default = disabled)\r\n",
      "  --dumpProfile               Print profile information per layer (default = disabled)\r\n",
      "  --dumpLayerInfo             Print layer information of the engine to console (default = disabled)\r\n",
      "  --exportTimes=<file>        Write the timing results in a json file (default = disabled)\r\n",
      "  --exportOutput=<file>       Write the output tensors to a json file (default = disabled)\r\n",
      "  --exportProfile=<file>      Write the profile information per layer in a json file (default = disabled)\r\n",
      "  --exportLayerInfo=<file>    Write the layer information of the engine in a json file (default = disabled)\r\n",
      "\r\n",
      "=== System Options ===\r\n",
      "  --device=N                  Select cuda device N (default = 0)\r\n",
      "  --useDLACore=N              Select DLA core N for layers that support DLA (default = none)\r\n",
      "  --allowGPUFallback          When DLA is enabled, allow GPU fallback for unsupported layers (default = disabled)\r\n",
      "  --plugins                   Plugin library (.so) to load (can be specified multiple times)\r\n",
      "\r\n",
      "=== Help ===\r\n",
      "  --help, -h                  Print this message\r\n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f34e134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494fa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ONNX_MODEL_DIR = './Flask/Models/'\n",
    "\n",
    "FLASK_MODEL_DIR = './Flask/Models/'\n",
    "\n",
    "TRITON_MODEL_DIR = './Triton/Models/'\n",
    "TRITON_CONFIG_FILE = 'config.pbtxt'\n",
    "TRITON_MODEL_FILE = 'model.plan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3386000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tensorrt_model(onnx_model, trt_model, precision, config):\n",
    "    trt_model = trt_model + '_' + precision\n",
    "    onnx_model_path = os.path.join(ONNX_MODEL_DIR, onnx_model + '.onnx')\n",
    "    flask_model_path = os.path.join(FLASK_MODEL_DIR, trt_model + '.plan')\n",
    "\n",
    "    !sudo /usr/src/tensorrt/bin/trtexec \\\n",
    "        --onnx=$onnx_model_path \\\n",
    "        --explicitBatch \\\n",
    "        --$precision \\\n",
    "        --saveEngine=$flask_model_path\n",
    "\n",
    "    triton_config_path = os.path.join(TRITON_MODEL_DIR, trt_model, TRITON_CONFIG_FILE)\n",
    "    os.makedirs(os.path.dirname(triton_config_path), exist_ok=True)\n",
    "    with open(triton_config_path, 'w') as f:\n",
    "        f.write(config.strip())\n",
    "\n",
    "    triton_model_path = os.path.join(TRITON_MODEL_DIR, trt_model, '1', TRITON_MODEL_FILE)\n",
    "    os.makedirs(os.path.dirname(triton_model_path), exist_ok=True)\n",
    "    !sudo cp $flask_model_path $triton_model_path\n",
    "        \n",
    "    print(flask_model_path)\n",
    "    print(triton_config_path)\n",
    "    print(triton_model_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9df9d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/resnet34-trt_best.plan\n",
      "[09/22/2022-21:23:15] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:23:15] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:23:15] [I] === Model Options ===\n",
      "[09/22/2022-21:23:15] [I] Format: ONNX\n",
      "[09/22/2022-21:23:15] [I] Model: ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:23:15] [I] Output:\n",
      "[09/22/2022-21:23:15] [I] === Build Options ===\n",
      "[09/22/2022-21:23:15] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:23:15] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:23:15] [I] minTiming: 1\n",
      "[09/22/2022-21:23:15] [I] avgTiming: 8\n",
      "[09/22/2022-21:23:15] [I] Precision: FP32+FP16+INT8\n",
      "[09/22/2022-21:23:15] [I] Calibration: Dynamic\n",
      "[09/22/2022-21:23:15] [I] Refit: Disabled\n",
      "[09/22/2022-21:23:15] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:23:15] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:23:15] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:23:15] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:23:15] [I] Save engine: ./Flask/Models/resnet34-trt_best.plan\n",
      "[09/22/2022-21:23:15] [I] Load engine: \n",
      "[09/22/2022-21:23:15] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:23:15] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:23:15] [I] timingCacheMode: local\n",
      "[09/22/2022-21:23:15] [I] timingCacheFile: \n",
      "[09/22/2022-21:23:15] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:23:15] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:23:15] [I] Input build shapes: model\n",
      "[09/22/2022-21:23:15] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:23:15] [I] === System Options ===\n",
      "[09/22/2022-21:23:15] [I] Device: 0\n",
      "[09/22/2022-21:23:15] [I] DLACore: \n",
      "[09/22/2022-21:23:15] [I] Plugins:\n",
      "[09/22/2022-21:23:15] [I] === Inference Options ===\n",
      "[09/22/2022-21:23:15] [I] Batch: Explicit\n",
      "[09/22/2022-21:23:15] [I] Input inference shapes: model\n",
      "[09/22/2022-21:23:15] [I] Iterations: 10\n",
      "[09/22/2022-21:23:15] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:23:15] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:23:15] [I] Idle time: 0ms\n",
      "[09/22/2022-21:23:15] [I] Streams: 1\n",
      "[09/22/2022-21:23:15] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:23:15] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:23:15] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:23:15] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:23:15] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:23:15] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:23:15] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:23:15] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:23:15] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:23:15] [I] Inputs:\n",
      "[09/22/2022-21:23:15] [I] === Reporting Options ===\n",
      "[09/22/2022-21:23:15] [I] Verbose: Disabled\n",
      "[09/22/2022-21:23:15] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:23:15] [I] Percentile: 99\n",
      "[09/22/2022-21:23:15] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:23:15] [I] Dump output: Disabled\n",
      "[09/22/2022-21:23:15] [I] Profile: Disabled\n",
      "[09/22/2022-21:23:15] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:23:15] [I] Export output to JSON file: \n",
      "[09/22/2022-21:23:15] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:23:15] [I] \n",
      "[09/22/2022-21:23:15] [I] === Device Information ===\n",
      "[09/22/2022-21:23:15] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:23:15] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:23:15] [I] SMs: 1\n",
      "[09/22/2022-21:23:15] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:23:15] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:23:15] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:23:15] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:23:15] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:23:15] [I] \n",
      "[09/22/2022-21:23:15] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:23:16] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2896 (MiB)\n",
      "[09/22/2022-21:23:17] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2924 MiB\n",
      "[09/22/2022-21:23:17] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2955 MiB\n",
      "[09/22/2022-21:23:17] [I] Start parsing network model\n",
      "[09/22/2022-21:23:17] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:23:17] [I] [TRT] Input filename:   ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:23:17] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:23:17] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:23:17] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:23:17] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:23:17] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:23:17] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:23:17] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:23:17] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:23:17] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:23:17] [I] Finish parsing network model\n",
      "[09/22/2022-21:23:17] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:23:17] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-21:23:17] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-21:23:17] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:23:17] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_0 + Relu_1\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] MaxPool_2\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_3 + Relu_4\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_5 + Add_6 + Relu_7\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_8 + Relu_9\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_10 + Add_11 + Relu_12\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_13 + Relu_14\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_15 + Add_16 + Relu_17\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_18 + Relu_19\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_20 + Add_22 + Relu_23\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_24 + Relu_25\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_26 + Add_27 + Relu_28\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_29 + Relu_30\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_31 + Add_32 + Relu_33\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_34 + Relu_35\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_36 + Add_37 + Relu_38\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_39 + Relu_40\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_42\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_41 + Add_43 + Relu_44\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_45 + Relu_46\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_47 + Add_48 + Relu_49\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_50 + Relu_51\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_52 + Add_53 + Relu_54\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_55 + Relu_56\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_57 + Add_58 + Relu_59\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_60 + Relu_61\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_62 + Add_63 + Relu_64\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_65 + Relu_66\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_67 + Add_68 + Relu_69\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_70 + Relu_71\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_73\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_72 + Add_74 + Relu_75\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_76 + Relu_77\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_78 + Add_79 + Relu_80\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_81 + Relu_82\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Conv_83 + Add_84 + Relu_85\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] GlobalAveragePool_86\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] Gemm_88\n",
      "[09/22/2022-21:23:17] [I] [TRT] [GpuLayer] (Unnamed Layer* 103) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:23:18] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +125, now: CPU 519, GPU 3202 (MiB)\n",
      "[09/22/2022-21:23:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +247, now: CPU 760, GPU 3449 (MiB)\n",
      "[09/22/2022-21:23:20] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:23:36] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:26:56] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:26:57] [I] [TRT] Total Host Persistent Memory: 38848\n",
      "[09/22/2022-21:26:57] [I] [TRT] Total Device Persistent Memory: 72795648\n",
      "[09/22/2022-21:26:57] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 45 MiB, GPU 192 MiB\n",
      "[09/22/2022-21:26:57] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.98267ms to assign 3 blocks to 41 nodes requiring 2408448 bytes.\n",
      "[09/22/2022-21:26:57] [I] [TRT] Total Activation Memory: 2408448\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +0, now: CPU 1050, GPU 3554 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 1050, GPU 3554 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +41, GPU +128, now: CPU 41, GPU 128 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1087, GPU 3596 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] Loaded engine size: 84 MiB\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1087, GPU 3592 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 1087, GPU 3592 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +83, now: CPU 0, GPU 83 (MiB)\n",
      "[09/22/2022-21:26:57] [I] Engine built in 222.182 sec.\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 890, GPU 3551 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 890, GPU 3551 (MiB)\n",
      "[09/22/2022-21:26:57] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +72, now: CPU 0, GPU 155 (MiB)\n",
      "[09/22/2022-21:26:57] [I] Using random values for input input__0\n",
      "[09/22/2022-21:26:57] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:26:57] [I] Using random values for output output__0\n",
      "[09/22/2022-21:26:57] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:26:57] [I] Starting inference\n",
      "[09/22/2022-21:27:00] [I] Warmup completed 10 queries over 200 ms\n",
      "[09/22/2022-21:27:00] [I] Timing trace has 154 queries over 3.04315 s\n",
      "[09/22/2022-21:27:00] [I] \n",
      "[09/22/2022-21:27:00] [I] === Trace details ===\n",
      "[09/22/2022-21:27:00] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5601 ms - Host latency: 19.62 ms (end to end 19.6296 ms, enqueue 1.33502 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.969 ms - Host latency: 20.0296 ms (end to end 20.0389 ms, enqueue 1.43179 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5339 ms - Host latency: 19.5935 ms (end to end 19.6029 ms, enqueue 1.33962 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.527 ms - Host latency: 19.5865 ms (end to end 19.596 ms, enqueue 1.34482 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5547 ms - Host latency: 19.6142 ms (end to end 19.6237 ms, enqueue 1.37253 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.8389 ms - Host latency: 19.9002 ms (end to end 19.943 ms, enqueue 1.43199 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5542 ms - Host latency: 19.6134 ms (end to end 19.6227 ms, enqueue 1.2796 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.8241 ms - Host latency: 19.8837 ms (end to end 19.8931 ms, enqueue 1.34962 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5701 ms - Host latency: 19.6301 ms (end to end 19.6398 ms, enqueue 1.41356 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.4207 ms - Host latency: 19.4807 ms (end to end 19.4902 ms, enqueue 1.377 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5227 ms - Host latency: 19.5822 ms (end to end 19.5917 ms, enqueue 1.32449 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 20.5668 ms - Host latency: 20.6271 ms (end to end 20.6367 ms, enqueue 1.44492 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.593 ms - Host latency: 19.6531 ms (end to end 19.6627 ms, enqueue 1.4656 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.8618 ms - Host latency: 19.9216 ms (end to end 19.9311 ms, enqueue 1.36782 ms)\n",
      "[09/22/2022-21:27:00] [I] Average on 10 runs - GPU latency: 19.5136 ms - Host latency: 19.5729 ms (end to end 19.5822 ms, enqueue 1.35137 ms)\n",
      "[09/22/2022-21:27:00] [I] \n",
      "[09/22/2022-21:27:00] [I] === Performance summary ===\n",
      "[09/22/2022-21:27:00] [I] Throughput: 50.6055 qps\n",
      "[09/22/2022-21:27:00] [I] Latency: min = 19.3151 ms, max = 24.0151 ms, mean = 19.7485 ms, median = 19.6284 ms, percentile(99%) = 23.2509 ms\n",
      "[09/22/2022-21:27:00] [I] End-to-End Host Latency: min = 19.3246 ms, max = 24.0249 ms, mean = 19.7601 ms, median = 19.6376 ms, percentile(99%) = 23.2604 ms\n",
      "[09/22/2022-21:27:00] [I] Enqueue Time: min = 1.16357 ms, max = 1.86963 ms, mean = 1.37555 ms, median = 1.35779 ms, percentile(99%) = 1.69043 ms\n",
      "[09/22/2022-21:27:00] [I] H2D Latency: min = 0.055542 ms, max = 0.0620117 ms, mean = 0.056959 ms, median = 0.0567932 ms, percentile(99%) = 0.0606689 ms\n",
      "[09/22/2022-21:27:00] [I] GPU Compute Time: min = 19.2557 ms, max = 23.9551 ms, mean = 19.6886 ms, median = 19.5678 ms, percentile(99%) = 23.1913 ms\n",
      "[09/22/2022-21:27:00] [I] D2H Latency: min = 0.00170898 ms, max = 0.00341797 ms, mean = 0.0028744 ms, median = 0.00292969 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-21:27:00] [I] Total Host Walltime: 3.04315 s\n",
      "[09/22/2022-21:27:00] [I] Total GPU Compute Time: 3.03205 s\n",
      "[09/22/2022-21:27:00] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:27:00] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/resnet34-trt_best.plan\n",
      "./Flask/Models/resnet34-trt_best.plan\n",
      "./Triton/Models/resnet34-trt_best/config.pbtxt\n",
      "./Triton/Models/resnet34-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx'\n",
    "trt_model = 'resnet34-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87cfdc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/resnet34-trt_fp16.plan\n",
      "[09/22/2022-21:27:02] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:27:02] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:27:02] [I] === Model Options ===\n",
      "[09/22/2022-21:27:02] [I] Format: ONNX\n",
      "[09/22/2022-21:27:02] [I] Model: ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:27:02] [I] Output:\n",
      "[09/22/2022-21:27:02] [I] === Build Options ===\n",
      "[09/22/2022-21:27:02] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:27:02] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:27:02] [I] minTiming: 1\n",
      "[09/22/2022-21:27:02] [I] avgTiming: 8\n",
      "[09/22/2022-21:27:02] [I] Precision: FP32+FP16\n",
      "[09/22/2022-21:27:02] [I] Calibration: \n",
      "[09/22/2022-21:27:02] [I] Refit: Disabled\n",
      "[09/22/2022-21:27:02] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:27:02] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:27:02] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:27:02] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:27:02] [I] Save engine: ./Flask/Models/resnet34-trt_fp16.plan\n",
      "[09/22/2022-21:27:02] [I] Load engine: \n",
      "[09/22/2022-21:27:02] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:27:02] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:27:02] [I] timingCacheMode: local\n",
      "[09/22/2022-21:27:02] [I] timingCacheFile: \n",
      "[09/22/2022-21:27:02] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:27:02] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:27:02] [I] Input build shapes: model\n",
      "[09/22/2022-21:27:02] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:27:02] [I] === System Options ===\n",
      "[09/22/2022-21:27:02] [I] Device: 0\n",
      "[09/22/2022-21:27:02] [I] DLACore: \n",
      "[09/22/2022-21:27:02] [I] Plugins:\n",
      "[09/22/2022-21:27:02] [I] === Inference Options ===\n",
      "[09/22/2022-21:27:02] [I] Batch: Explicit\n",
      "[09/22/2022-21:27:02] [I] Input inference shapes: model\n",
      "[09/22/2022-21:27:02] [I] Iterations: 10\n",
      "[09/22/2022-21:27:02] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:27:02] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:27:02] [I] Idle time: 0ms\n",
      "[09/22/2022-21:27:02] [I] Streams: 1\n",
      "[09/22/2022-21:27:02] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:27:02] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:27:02] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:27:02] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:27:02] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:27:02] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:27:02] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:27:02] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:27:02] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:27:02] [I] Inputs:\n",
      "[09/22/2022-21:27:02] [I] === Reporting Options ===\n",
      "[09/22/2022-21:27:02] [I] Verbose: Disabled\n",
      "[09/22/2022-21:27:02] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:27:02] [I] Percentile: 99\n",
      "[09/22/2022-21:27:02] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:27:02] [I] Dump output: Disabled\n",
      "[09/22/2022-21:27:02] [I] Profile: Disabled\n",
      "[09/22/2022-21:27:02] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:27:02] [I] Export output to JSON file: \n",
      "[09/22/2022-21:27:02] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:27:02] [I] \n",
      "[09/22/2022-21:27:02] [I] === Device Information ===\n",
      "[09/22/2022-21:27:02] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:27:02] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:27:02] [I] SMs: 1\n",
      "[09/22/2022-21:27:02] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:27:02] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:27:02] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:27:02] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:27:02] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:27:02] [I] \n",
      "[09/22/2022-21:27:02] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:27:03] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2935 (MiB)\n",
      "[09/22/2022-21:27:04] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2965 MiB\n",
      "[09/22/2022-21:27:04] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2993 MiB\n",
      "[09/22/2022-21:27:04] [I] Start parsing network model\n",
      "[09/22/2022-21:27:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:27:06] [I] [TRT] Input filename:   ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:27:06] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:27:06] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:27:06] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:27:06] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:27:06] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:27:06] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:27:06] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:27:06] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:27:06] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:27:06] [I] Finish parsing network model\n",
      "[09/22/2022-21:27:06] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:27:06] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:27:06] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_0 + Relu_1\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] MaxPool_2\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_3 + Relu_4\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_5 + Add_6 + Relu_7\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_8 + Relu_9\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_10 + Add_11 + Relu_12\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_13 + Relu_14\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_15 + Add_16 + Relu_17\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_18 + Relu_19\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_20 + Add_22 + Relu_23\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_24 + Relu_25\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_26 + Add_27 + Relu_28\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_29 + Relu_30\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_31 + Add_32 + Relu_33\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_34 + Relu_35\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_36 + Add_37 + Relu_38\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_39 + Relu_40\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_42\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_41 + Add_43 + Relu_44\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_45 + Relu_46\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_47 + Add_48 + Relu_49\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_50 + Relu_51\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_52 + Add_53 + Relu_54\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_55 + Relu_56\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_57 + Add_58 + Relu_59\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_60 + Relu_61\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_62 + Add_63 + Relu_64\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_65 + Relu_66\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_67 + Add_68 + Relu_69\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_70 + Relu_71\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_73\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_72 + Add_74 + Relu_75\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_76 + Relu_77\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_78 + Add_79 + Relu_80\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_81 + Relu_82\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Conv_83 + Add_84 + Relu_85\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] GlobalAveragePool_86\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] Gemm_88\n",
      "[09/22/2022-21:27:06] [I] [TRT] [GpuLayer] (Unnamed Layer* 103) [Shuffle]\n",
      "[09/22/2022-21:27:07] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +122, now: CPU 519, GPU 3321 (MiB)\n",
      "[09/22/2022-21:27:08] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +243, now: CPU 760, GPU 3564 (MiB)\n",
      "[09/22/2022-21:27:08] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:27:20] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:28:44] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:28:45] [I] [TRT] Total Host Persistent Memory: 37312\n",
      "[09/22/2022-21:28:45] [I] [TRT] Total Device Persistent Memory: 72795648\n",
      "[09/22/2022-21:28:45] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 45 MiB, GPU 192 MiB\n",
      "[09/22/2022-21:28:45] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 3.05637ms to assign 3 blocks to 41 nodes requiring 2408448 bytes.\n",
      "[09/22/2022-21:28:45] [I] [TRT] Total Activation Memory: 2408448\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1049, GPU 3622 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +0, now: CPU 1050, GPU 3622 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +41, GPU +128, now: CPU 41, GPU 128 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1087, GPU 3597 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] Loaded engine size: 84 MiB\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1087, GPU 3587 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 1087, GPU 3587 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +83, now: CPU 0, GPU 83 (MiB)\n",
      "[09/22/2022-21:28:45] [I] Engine built in 103.618 sec.\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 890, GPU 3543 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 890, GPU 3543 (MiB)\n",
      "[09/22/2022-21:28:45] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +72, now: CPU 0, GPU 155 (MiB)\n",
      "[09/22/2022-21:28:45] [I] Using random values for input input__0\n",
      "[09/22/2022-21:28:45] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:28:45] [I] Using random values for output output__0\n",
      "[09/22/2022-21:28:45] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:28:45] [I] Starting inference\n",
      "[09/22/2022-21:28:49] [I] Warmup completed 10 queries over 200 ms\n",
      "[09/22/2022-21:28:49] [I] Timing trace has 154 queries over 3.03564 s\n",
      "[09/22/2022-21:28:49] [I] \n",
      "[09/22/2022-21:28:49] [I] === Trace details ===\n",
      "[09/22/2022-21:28:49] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.8857 ms - Host latency: 19.9455 ms (end to end 19.9551 ms, enqueue 1.55211 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.4697 ms - Host latency: 19.5286 ms (end to end 19.5382 ms, enqueue 1.34562 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.463 ms - Host latency: 19.522 ms (end to end 19.5314 ms, enqueue 1.32268 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5931 ms - Host latency: 19.6532 ms (end to end 19.6629 ms, enqueue 1.49659 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.8601 ms - Host latency: 19.9202 ms (end to end 19.9296 ms, enqueue 1.42678 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5476 ms - Host latency: 19.6068 ms (end to end 19.6162 ms, enqueue 1.33413 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.8584 ms - Host latency: 19.917 ms (end to end 19.9265 ms, enqueue 1.34041 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.6315 ms - Host latency: 19.6921 ms (end to end 19.7015 ms, enqueue 1.50414 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5183 ms - Host latency: 19.5773 ms (end to end 19.5869 ms, enqueue 1.51729 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5183 ms - Host latency: 19.5776 ms (end to end 19.5872 ms, enqueue 1.30022 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5526 ms - Host latency: 19.612 ms (end to end 19.6217 ms, enqueue 1.36504 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.9365 ms - Host latency: 19.9965 ms (end to end 20.006 ms, enqueue 1.55183 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.5073 ms - Host latency: 19.5664 ms (end to end 19.5758 ms, enqueue 1.35854 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.7527 ms - Host latency: 19.8119 ms (end to end 19.8214 ms, enqueue 1.27459 ms)\n",
      "[09/22/2022-21:28:49] [I] Average on 10 runs - GPU latency: 19.6133 ms - Host latency: 19.6732 ms (end to end 19.6827 ms, enqueue 1.48665 ms)\n",
      "[09/22/2022-21:28:49] [I] \n",
      "[09/22/2022-21:28:49] [I] === Performance summary ===\n",
      "[09/22/2022-21:28:49] [I] Throughput: 50.7306 qps\n",
      "[09/22/2022-21:28:49] [I] Latency: min = 19.2935 ms, max = 23.1577 ms, mean = 19.7019 ms, median = 19.5958 ms, percentile(99%) = 23.0447 ms\n",
      "[09/22/2022-21:28:49] [I] End-to-End Host Latency: min = 19.3033 ms, max = 23.1672 ms, mean = 19.7114 ms, median = 19.6055 ms, percentile(99%) = 23.0548 ms\n",
      "[09/22/2022-21:28:49] [I] Enqueue Time: min = 1.17114 ms, max = 2.85706 ms, mean = 1.41215 ms, median = 1.37634 ms, percentile(99%) = 2.36499 ms\n",
      "[09/22/2022-21:28:49] [I] H2D Latency: min = 0.0549316 ms, max = 0.0605469 ms, mean = 0.0566642 ms, median = 0.0563965 ms, percentile(99%) = 0.0597534 ms\n",
      "[09/22/2022-21:28:49] [I] GPU Compute Time: min = 19.2334 ms, max = 23.0979 ms, mean = 19.6424 ms, median = 19.5354 ms, percentile(99%) = 22.9841 ms\n",
      "[09/22/2022-21:28:49] [I] D2H Latency: min = 0.00146484 ms, max = 0.00341797 ms, mean = 0.00281723 ms, median = 0.00282288 ms, percentile(99%) = 0.0032959 ms\n",
      "[09/22/2022-21:28:49] [I] Total Host Walltime: 3.03564 s\n",
      "[09/22/2022-21:28:49] [I] Total GPU Compute Time: 3.02493 s\n",
      "[09/22/2022-21:28:49] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:28:49] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/resnet34-trt_fp16.plan\n",
      "./Flask/Models/resnet34-trt_fp16.plan\n",
      "./Triton/Models/resnet34-trt_fp16/config.pbtxt\n",
      "./Triton/Models/resnet34-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx'\n",
    "trt_model = 'resnet34-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa99ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/resnet34-trt_int8.plan\n",
      "[09/22/2022-21:28:52] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:28:52] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:28:52] [I] === Model Options ===\n",
      "[09/22/2022-21:28:52] [I] Format: ONNX\n",
      "[09/22/2022-21:28:52] [I] Model: ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:28:52] [I] Output:\n",
      "[09/22/2022-21:28:52] [I] === Build Options ===\n",
      "[09/22/2022-21:28:52] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:28:52] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:28:52] [I] minTiming: 1\n",
      "[09/22/2022-21:28:52] [I] avgTiming: 8\n",
      "[09/22/2022-21:28:52] [I] Precision: FP32+INT8\n",
      "[09/22/2022-21:28:52] [I] Calibration: Dynamic\n",
      "[09/22/2022-21:28:52] [I] Refit: Disabled\n",
      "[09/22/2022-21:28:52] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:28:52] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:28:52] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:28:52] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:28:52] [I] Save engine: ./Flask/Models/resnet34-trt_int8.plan\n",
      "[09/22/2022-21:28:52] [I] Load engine: \n",
      "[09/22/2022-21:28:52] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:28:52] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:28:52] [I] timingCacheMode: local\n",
      "[09/22/2022-21:28:52] [I] timingCacheFile: \n",
      "[09/22/2022-21:28:52] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:28:52] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:28:52] [I] Input build shapes: model\n",
      "[09/22/2022-21:28:52] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:28:52] [I] === System Options ===\n",
      "[09/22/2022-21:28:52] [I] Device: 0\n",
      "[09/22/2022-21:28:52] [I] DLACore: \n",
      "[09/22/2022-21:28:52] [I] Plugins:\n",
      "[09/22/2022-21:28:52] [I] === Inference Options ===\n",
      "[09/22/2022-21:28:52] [I] Batch: Explicit\n",
      "[09/22/2022-21:28:52] [I] Input inference shapes: model\n",
      "[09/22/2022-21:28:52] [I] Iterations: 10\n",
      "[09/22/2022-21:28:52] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:28:52] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:28:52] [I] Idle time: 0ms\n",
      "[09/22/2022-21:28:52] [I] Streams: 1\n",
      "[09/22/2022-21:28:52] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:28:52] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:28:52] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:28:52] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:28:52] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:28:52] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:28:52] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:28:52] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:28:52] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:28:52] [I] Inputs:\n",
      "[09/22/2022-21:28:52] [I] === Reporting Options ===\n",
      "[09/22/2022-21:28:52] [I] Verbose: Disabled\n",
      "[09/22/2022-21:28:52] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:28:52] [I] Percentile: 99\n",
      "[09/22/2022-21:28:52] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:28:52] [I] Dump output: Disabled\n",
      "[09/22/2022-21:28:52] [I] Profile: Disabled\n",
      "[09/22/2022-21:28:52] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:28:52] [I] Export output to JSON file: \n",
      "[09/22/2022-21:28:52] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:28:52] [I] \n",
      "[09/22/2022-21:28:52] [I] === Device Information ===\n",
      "[09/22/2022-21:28:52] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:28:52] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:28:52] [I] SMs: 1\n",
      "[09/22/2022-21:28:52] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:28:52] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:28:52] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:28:52] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:28:52] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:28:52] [I] \n",
      "[09/22/2022-21:28:52] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:28:53] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2900 (MiB)\n",
      "[09/22/2022-21:28:54] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2901 MiB\n",
      "[09/22/2022-21:28:54] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 277 MiB, GPU 2929 MiB\n",
      "[09/22/2022-21:28:54] [I] Start parsing network model\n",
      "[09/22/2022-21:28:54] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:28:54] [I] [TRT] Input filename:   ./Flask/Models/resnet34-onnx.onnx\n",
      "[09/22/2022-21:28:54] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:28:54] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:28:54] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:28:54] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:28:54] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:28:54] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:28:54] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:28:54] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:28:54] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:28:54] [I] Finish parsing network model\n",
      "[09/22/2022-21:28:54] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:28:54] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best\n",
      "[09/22/2022-21:28:54] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-21:28:54] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-21:28:54] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:28:54] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_0 + Relu_1\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] MaxPool_2\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_3 + Relu_4\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_5 + Add_6 + Relu_7\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_8 + Relu_9\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_10 + Add_11 + Relu_12\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_13 + Relu_14\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_15 + Add_16 + Relu_17\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_18 + Relu_19\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_20 + Add_22 + Relu_23\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_24 + Relu_25\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_26 + Add_27 + Relu_28\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_29 + Relu_30\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_31 + Add_32 + Relu_33\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_34 + Relu_35\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_36 + Add_37 + Relu_38\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_39 + Relu_40\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_42\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_41 + Add_43 + Relu_44\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_45 + Relu_46\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_47 + Add_48 + Relu_49\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_50 + Relu_51\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_52 + Add_53 + Relu_54\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_55 + Relu_56\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_57 + Add_58 + Relu_59\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_60 + Relu_61\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_62 + Add_63 + Relu_64\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_65 + Relu_66\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_67 + Add_68 + Relu_69\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_70 + Relu_71\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_73\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_72 + Add_74 + Relu_75\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_76 + Relu_77\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_78 + Add_79 + Relu_80\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_81 + Relu_82\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Conv_83 + Add_84 + Relu_85\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] GlobalAveragePool_86\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] Gemm_88\n",
      "[09/22/2022-21:28:54] [I] [TRT] [GpuLayer] (Unnamed Layer* 103) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:28:55] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +126, now: CPU 519, GPU 3177 (MiB)\n",
      "[09/22/2022-21:28:56] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +236, now: CPU 760, GPU 3413 (MiB)\n",
      "[09/22/2022-21:28:56] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:29:05] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:31:53] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:31:54] [I] [TRT] Total Host Persistent Memory: 37584\n",
      "[09/22/2022-21:31:54] [I] [TRT] Total Device Persistent Memory: 140762112\n",
      "[09/22/2022-21:31:54] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 384 MiB\n",
      "[09/22/2022-21:31:54] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 2.70418ms to assign 3 blocks to 39 nodes requiring 4816896 bytes.\n",
      "[09/22/2022-21:31:54] [I] [TRT] Total Activation Memory: 4816896\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1012, GPU 3535 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +0, now: CPU 1013, GPU 3535 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +256, now: CPU 0, GPU 256 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 1180, GPU 3641 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] Loaded engine size: 167 MiB\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 1180, GPU 3604 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 1180, GPU 3604 (MiB)\n",
      "[09/22/2022-21:31:54] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +167, now: CPU 0, GPU 167 (MiB)\n",
      "[09/22/2022-21:31:55] [I] Engine built in 183.611 sec.\n",
      "[09/22/2022-21:31:56] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 899, GPU 3447 (MiB)\n",
      "[09/22/2022-21:31:56] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 899, GPU 3447 (MiB)\n",
      "[09/22/2022-21:31:56] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +139, now: CPU 0, GPU 306 (MiB)\n",
      "[09/22/2022-21:31:56] [I] Using random values for input input__0\n",
      "[09/22/2022-21:31:56] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:31:56] [I] Using random values for output output__0\n",
      "[09/22/2022-21:31:56] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:31:56] [I] Starting inference\n",
      "[09/22/2022-21:31:59] [I] Warmup completed 6 queries over 200 ms\n",
      "[09/22/2022-21:31:59] [I] Timing trace has 89 queries over 3.04436 s\n",
      "[09/22/2022-21:31:59] [I] \n",
      "[09/22/2022-21:31:59] [I] === Trace details ===\n",
      "[09/22/2022-21:31:59] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 33.9979 ms - Host latency: 34.0589 ms (end to end 34.0685 ms, enqueue 1.41205 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 33.9626 ms - Host latency: 34.0227 ms (end to end 34.0322 ms, enqueue 1.49197 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 34.3513 ms - Host latency: 34.4112 ms (end to end 34.4208 ms, enqueue 1.36813 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 34.2923 ms - Host latency: 34.3525 ms (end to end 34.3621 ms, enqueue 1.43577 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 33.9259 ms - Host latency: 33.986 ms (end to end 33.9954 ms, enqueue 1.35144 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 34.0477 ms - Host latency: 34.1076 ms (end to end 34.4253 ms, enqueue 1.4933 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 34.2807 ms - Host latency: 34.3409 ms (end to end 34.3506 ms, enqueue 1.3667 ms)\n",
      "[09/22/2022-21:31:59] [I] Average on 10 runs - GPU latency: 34.0732 ms - Host latency: 34.1338 ms (end to end 34.1432 ms, enqueue 1.45088 ms)\n",
      "[09/22/2022-21:31:59] [I] \n",
      "[09/22/2022-21:31:59] [I] === Performance summary ===\n",
      "[09/22/2022-21:31:59] [I] Throughput: 29.2344 qps\n",
      "[09/22/2022-21:31:59] [I] Latency: min = 33.6736 ms, max = 37.5782 ms, mean = 34.1616 ms, median = 34.0588 ms, percentile(99%) = 37.5782 ms\n",
      "[09/22/2022-21:31:59] [I] End-to-End Host Latency: min = 33.683 ms, max = 37.5878 ms, mean = 34.2057 ms, median = 34.078 ms, percentile(99%) = 37.5878 ms\n",
      "[09/22/2022-21:31:59] [I] Enqueue Time: min = 1.22363 ms, max = 2.3219 ms, mean = 1.41168 ms, median = 1.38477 ms, percentile(99%) = 2.3219 ms\n",
      "[09/22/2022-21:31:59] [I] H2D Latency: min = 0.0554199 ms, max = 0.0610352 ms, mean = 0.0572956 ms, median = 0.0569153 ms, percentile(99%) = 0.0610352 ms\n",
      "[09/22/2022-21:31:59] [I] GPU Compute Time: min = 33.6139 ms, max = 37.5192 ms, mean = 34.1014 ms, median = 34.001 ms, percentile(99%) = 37.5192 ms\n",
      "[09/22/2022-21:31:59] [I] D2H Latency: min = 0.00195312 ms, max = 0.00341797 ms, mean = 0.00288408 ms, median = 0.00292969 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-21:31:59] [I] Total Host Walltime: 3.04436 s\n",
      "[09/22/2022-21:31:59] [I] Total GPU Compute Time: 3.03502 s\n",
      "[09/22/2022-21:31:59] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:31:59] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/resnet34-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/resnet34-trt_int8.plan\n",
      "./Flask/Models/resnet34-trt_int8.plan\n",
      "./Triton/Models/resnet34-trt_int8/config.pbtxt\n",
      "./Triton/Models/resnet34-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'resnet34-onnx'\n",
    "trt_model = 'resnet34-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221adaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/mobilenet_v2-trt_best.plan\n",
      "[09/22/2022-21:32:07] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:32:07] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:32:07] [I] === Model Options ===\n",
      "[09/22/2022-21:32:07] [I] Format: ONNX\n",
      "[09/22/2022-21:32:07] [I] Model: ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:32:07] [I] Output:\n",
      "[09/22/2022-21:32:07] [I] === Build Options ===\n",
      "[09/22/2022-21:32:07] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:32:07] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:32:07] [I] minTiming: 1\n",
      "[09/22/2022-21:32:07] [I] avgTiming: 8\n",
      "[09/22/2022-21:32:07] [I] Precision: FP32+FP16+INT8\n",
      "[09/22/2022-21:32:07] [I] Calibration: Dynamic\n",
      "[09/22/2022-21:32:07] [I] Refit: Disabled\n",
      "[09/22/2022-21:32:07] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:32:07] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:32:07] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:32:07] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:32:07] [I] Save engine: ./Flask/Models/mobilenet_v2-trt_best.plan\n",
      "[09/22/2022-21:32:07] [I] Load engine: \n",
      "[09/22/2022-21:32:07] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:32:07] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:32:07] [I] timingCacheMode: local\n",
      "[09/22/2022-21:32:07] [I] timingCacheFile: \n",
      "[09/22/2022-21:32:07] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:32:07] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:32:07] [I] Input build shapes: model\n",
      "[09/22/2022-21:32:07] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:32:07] [I] === System Options ===\n",
      "[09/22/2022-21:32:07] [I] Device: 0\n",
      "[09/22/2022-21:32:07] [I] DLACore: \n",
      "[09/22/2022-21:32:07] [I] Plugins:\n",
      "[09/22/2022-21:32:07] [I] === Inference Options ===\n",
      "[09/22/2022-21:32:07] [I] Batch: Explicit\n",
      "[09/22/2022-21:32:07] [I] Input inference shapes: model\n",
      "[09/22/2022-21:32:07] [I] Iterations: 10\n",
      "[09/22/2022-21:32:07] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:32:07] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:32:07] [I] Idle time: 0ms\n",
      "[09/22/2022-21:32:07] [I] Streams: 1\n",
      "[09/22/2022-21:32:07] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:32:07] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:32:07] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:32:07] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:32:07] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:32:07] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:32:07] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:32:07] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:32:07] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:32:07] [I] Inputs:\n",
      "[09/22/2022-21:32:07] [I] === Reporting Options ===\n",
      "[09/22/2022-21:32:07] [I] Verbose: Disabled\n",
      "[09/22/2022-21:32:07] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:32:07] [I] Percentile: 99\n",
      "[09/22/2022-21:32:07] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:32:07] [I] Dump output: Disabled\n",
      "[09/22/2022-21:32:07] [I] Profile: Disabled\n",
      "[09/22/2022-21:32:07] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:32:07] [I] Export output to JSON file: \n",
      "[09/22/2022-21:32:07] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:32:07] [I] \n",
      "[09/22/2022-21:32:07] [I] === Device Information ===\n",
      "[09/22/2022-21:32:07] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:32:07] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:32:07] [I] SMs: 1\n",
      "[09/22/2022-21:32:07] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:32:07] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:32:07] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:32:07] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:32:07] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:32:07] [I] \n",
      "[09/22/2022-21:32:07] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:32:08] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2867 (MiB)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2866 MiB\n",
      "[09/22/2022-21:32:09] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2894 MiB\n",
      "[09/22/2022-21:32:09] [I] Start parsing network model\n",
      "[09/22/2022-21:32:09] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:32:09] [I] [TRT] Input filename:   ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:32:09] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:32:09] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:32:09] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:32:09] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:32:09] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:32:09] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:32:09] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:32:09] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:32:09] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:32:09] [I] Finish parsing network model\n",
      "[09/22/2022-21:32:09] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:32:09] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-21:32:09] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-21:32:09] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:32:09] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_0 + PWN(Clip_1)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_2 + PWN(Clip_3)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_4\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_5 + PWN(Clip_6)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_7 + PWN(Clip_8)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_9\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_10 + PWN(Clip_11)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_12 + PWN(Clip_13)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_14 + Add_15\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_16 + PWN(Clip_17)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_18 + PWN(Clip_19)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_20\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_21 + PWN(Clip_22)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_23 + PWN(Clip_24)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_25 + Add_26\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_27 + PWN(Clip_28)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_29 + PWN(Clip_30)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_31 + Add_32\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_33 + PWN(Clip_34)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_35 + PWN(Clip_36)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_37\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_38 + PWN(Clip_39)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_40 + PWN(Clip_41)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_42 + Add_43\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_44 + PWN(Clip_45)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_46 + PWN(Clip_47)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_48 + Add_49\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_50 + PWN(Clip_51)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_52 + PWN(Clip_53)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_54 + Add_55\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_56 + PWN(Clip_57)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_58 + PWN(Clip_59)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_61 + PWN(Clip_62)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_63 + PWN(Clip_64)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_65 + Add_66\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_67 + PWN(Clip_68)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_69 + PWN(Clip_70)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_71 + Add_72\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_73 + PWN(Clip_74)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_75 + PWN(Clip_76)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_77\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_78 + PWN(Clip_79)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_80 + PWN(Clip_81)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_82 + Add_83\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_84 + PWN(Clip_85)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_86 + PWN(Clip_87)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_88 + Add_89\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_90 + PWN(Clip_91)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_92 + PWN(Clip_93)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_94\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Conv_95 + PWN(Clip_96)\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] GlobalAveragePool_97\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] Gemm_99\n",
      "[09/22/2022-21:32:09] [I] [TRT] [GpuLayer] (Unnamed Layer* 114) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:32:10] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +145, now: CPU 450, GPU 3081 (MiB)\n",
      "[09/22/2022-21:32:12] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +243, now: CPU 690, GPU 3324 (MiB)\n",
      "[09/22/2022-21:32:12] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:32:52] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:36:39] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:36:39] [I] [TRT] Total Host Persistent Memory: 103152\n",
      "[09/22/2022-21:36:39] [I] [TRT] Total Device Persistent Memory: 4304896\n",
      "[09/22/2022-21:36:39] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 32 MiB\n",
      "[09/22/2022-21:36:39] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 9.39574ms to assign 4 blocks to 68 nodes requiring 7024640 bytes.\n",
      "[09/22/2022-21:36:39] [I] [TRT] Total Activation Memory: 7024640\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 943, GPU 3594 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 943, GPU 3594 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +6, GPU +8, now: CPU 6, GPU 8 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 941, GPU 3599 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] Loaded engine size: 7 MiB\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +0, now: CPU 942, GPU 3599 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 942, GPU 3599 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +7, now: CPU 0, GPU 7 (MiB)\n",
      "[09/22/2022-21:36:39] [I] Engine built in 272.68 sec.\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 898, GPU 3607 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 898, GPU 3607 (MiB)\n",
      "[09/22/2022-21:36:39] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +11, now: CPU 0, GPU 18 (MiB)\n",
      "[09/22/2022-21:36:39] [I] Using random values for input input__0\n",
      "[09/22/2022-21:36:39] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:36:39] [I] Using random values for output output__0\n",
      "[09/22/2022-21:36:39] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:36:39] [I] Starting inference\n",
      "[09/22/2022-21:36:43] [I] Warmup completed 16 queries over 200 ms\n",
      "[09/22/2022-21:36:43] [I] Timing trace has 246 queries over 3.03237 s\n",
      "[09/22/2022-21:36:43] [I] \n",
      "[09/22/2022-21:36:43] [I] === Trace details ===\n",
      "[09/22/2022-21:36:43] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1963 ms - Host latency: 12.2557 ms (end to end 12.2652 ms, enqueue 3.32531 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.193 ms - Host latency: 12.2526 ms (end to end 12.2623 ms, enqueue 3.23233 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.256 ms - Host latency: 12.3163 ms (end to end 12.326 ms, enqueue 3.75634 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.2268 ms - Host latency: 12.2866 ms (end to end 12.2962 ms, enqueue 4.05901 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.172 ms - Host latency: 12.2314 ms (end to end 12.2409 ms, enqueue 3.35402 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1823 ms - Host latency: 12.2415 ms (end to end 12.2511 ms, enqueue 3.29026 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.5013 ms - Host latency: 12.5607 ms (end to end 12.5704 ms, enqueue 3.35068 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1846 ms - Host latency: 12.2441 ms (end to end 12.2536 ms, enqueue 3.31201 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.2557 ms - Host latency: 12.3168 ms (end to end 12.3264 ms, enqueue 4.02626 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.558 ms - Host latency: 12.6181 ms (end to end 12.6277 ms, enqueue 3.9979 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1844 ms - Host latency: 12.2438 ms (end to end 12.2531 ms, enqueue 3.498 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1848 ms - Host latency: 12.244 ms (end to end 12.2535 ms, enqueue 3.4352 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1756 ms - Host latency: 12.2349 ms (end to end 12.2444 ms, enqueue 3.42261 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.2243 ms - Host latency: 12.2836 ms (end to end 12.2933 ms, enqueue 3.45862 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.2415 ms - Host latency: 12.3023 ms (end to end 12.3118 ms, enqueue 4.21793 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1819 ms - Host latency: 12.2413 ms (end to end 12.251 ms, enqueue 3.82607 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.4955 ms - Host latency: 12.555 ms (end to end 12.5647 ms, enqueue 3.33259 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1959 ms - Host latency: 12.2552 ms (end to end 12.2646 ms, enqueue 3.20437 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1972 ms - Host latency: 12.2564 ms (end to end 12.2659 ms, enqueue 3.15737 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.6054 ms - Host latency: 12.6661 ms (end to end 12.6757 ms, enqueue 3.83594 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.2347 ms - Host latency: 12.2951 ms (end to end 12.3044 ms, enqueue 3.89314 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1874 ms - Host latency: 12.2466 ms (end to end 12.2561 ms, enqueue 3.50847 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1957 ms - Host latency: 12.255 ms (end to end 12.2646 ms, enqueue 3.23491 ms)\n",
      "[09/22/2022-21:36:43] [I] Average on 10 runs - GPU latency: 12.1758 ms - Host latency: 12.2353 ms (end to end 12.2449 ms, enqueue 3.21506 ms)\n",
      "[09/22/2022-21:36:43] [I] \n",
      "[09/22/2022-21:36:43] [I] === Performance summary ===\n",
      "[09/22/2022-21:36:43] [I] Throughput: 81.1248 qps\n",
      "[09/22/2022-21:36:43] [I] Latency: min = 12.1798 ms, max = 16.053 ms, mean = 12.3166 ms, median = 12.2578 ms, percentile(99%) = 15.4152 ms\n",
      "[09/22/2022-21:36:43] [I] End-to-End Host Latency: min = 12.1892 ms, max = 16.0635 ms, mean = 12.3261 ms, median = 12.2671 ms, percentile(99%) = 15.426 ms\n",
      "[09/22/2022-21:36:43] [I] Enqueue Time: min = 2.92871 ms, max = 5.94409 ms, mean = 3.53031 ms, median = 3.40546 ms, percentile(99%) = 4.75439 ms\n",
      "[09/22/2022-21:36:43] [I] H2D Latency: min = 0.0551758 ms, max = 0.0631104 ms, mean = 0.0568057 ms, median = 0.0566406 ms, percentile(99%) = 0.0603027 ms\n",
      "[09/22/2022-21:36:43] [I] GPU Compute Time: min = 12.1206 ms, max = 15.9905 ms, mean = 12.2569 ms, median = 12.1974 ms, percentile(99%) = 15.3561 ms\n",
      "[09/22/2022-21:36:43] [I] D2H Latency: min = 0.00170898 ms, max = 0.00354004 ms, mean = 0.00284397 ms, median = 0.00282288 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-21:36:43] [I] Total Host Walltime: 3.03237 s\n",
      "[09/22/2022-21:36:43] [I] Total GPU Compute Time: 3.0152 s\n",
      "[09/22/2022-21:36:43] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:36:43] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/mobilenet_v2-trt_best.plan\n",
      "./Flask/Models/mobilenet_v2-trt_best.plan\n",
      "./Triton/Models/mobilenet_v2-trt_best/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c84da281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/mobilenet_v2-trt_fp16.plan\n",
      "[09/22/2022-21:36:44] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:36:44] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:36:44] [I] === Model Options ===\n",
      "[09/22/2022-21:36:44] [I] Format: ONNX\n",
      "[09/22/2022-21:36:44] [I] Model: ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:36:44] [I] Output:\n",
      "[09/22/2022-21:36:44] [I] === Build Options ===\n",
      "[09/22/2022-21:36:44] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:36:44] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:36:44] [I] minTiming: 1\n",
      "[09/22/2022-21:36:44] [I] avgTiming: 8\n",
      "[09/22/2022-21:36:44] [I] Precision: FP32+FP16\n",
      "[09/22/2022-21:36:44] [I] Calibration: \n",
      "[09/22/2022-21:36:44] [I] Refit: Disabled\n",
      "[09/22/2022-21:36:44] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:36:44] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:36:44] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:36:44] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:36:44] [I] Save engine: ./Flask/Models/mobilenet_v2-trt_fp16.plan\n",
      "[09/22/2022-21:36:44] [I] Load engine: \n",
      "[09/22/2022-21:36:44] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:36:44] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:36:44] [I] timingCacheMode: local\n",
      "[09/22/2022-21:36:44] [I] timingCacheFile: \n",
      "[09/22/2022-21:36:44] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:36:44] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:36:44] [I] Input build shapes: model\n",
      "[09/22/2022-21:36:44] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:36:44] [I] === System Options ===\n",
      "[09/22/2022-21:36:44] [I] Device: 0\n",
      "[09/22/2022-21:36:44] [I] DLACore: \n",
      "[09/22/2022-21:36:44] [I] Plugins:\n",
      "[09/22/2022-21:36:44] [I] === Inference Options ===\n",
      "[09/22/2022-21:36:44] [I] Batch: Explicit\n",
      "[09/22/2022-21:36:44] [I] Input inference shapes: model\n",
      "[09/22/2022-21:36:44] [I] Iterations: 10\n",
      "[09/22/2022-21:36:44] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:36:44] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:36:44] [I] Idle time: 0ms\n",
      "[09/22/2022-21:36:44] [I] Streams: 1\n",
      "[09/22/2022-21:36:44] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:36:44] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:36:44] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:36:44] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:36:44] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:36:44] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:36:44] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:36:44] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:36:44] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:36:44] [I] Inputs:\n",
      "[09/22/2022-21:36:44] [I] === Reporting Options ===\n",
      "[09/22/2022-21:36:44] [I] Verbose: Disabled\n",
      "[09/22/2022-21:36:44] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:36:44] [I] Percentile: 99\n",
      "[09/22/2022-21:36:44] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:36:44] [I] Dump output: Disabled\n",
      "[09/22/2022-21:36:44] [I] Profile: Disabled\n",
      "[09/22/2022-21:36:44] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:36:44] [I] Export output to JSON file: \n",
      "[09/22/2022-21:36:44] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:36:44] [I] \n",
      "[09/22/2022-21:36:44] [I] === Device Information ===\n",
      "[09/22/2022-21:36:44] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:36:44] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:36:44] [I] SMs: 1\n",
      "[09/22/2022-21:36:44] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:36:44] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:36:44] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:36:44] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:36:44] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:36:44] [I] \n",
      "[09/22/2022-21:36:44] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:36:45] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2891 (MiB)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2890 MiB\n",
      "[09/22/2022-21:36:46] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2918 MiB\n",
      "[09/22/2022-21:36:46] [I] Start parsing network model\n",
      "[09/22/2022-21:36:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:36:46] [I] [TRT] Input filename:   ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:36:46] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:36:46] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:36:46] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:36:46] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:36:46] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:36:46] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:36:46] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:36:46] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:36:46] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:36:46] [I] Finish parsing network model\n",
      "[09/22/2022-21:36:46] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:36:46] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:36:46] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_0 + PWN(Clip_1)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_2 + PWN(Clip_3)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_4\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_5 + PWN(Clip_6)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_7 + PWN(Clip_8)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_9\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_10 + PWN(Clip_11)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_12 + PWN(Clip_13)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_14 + Add_15\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_16 + PWN(Clip_17)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_18 + PWN(Clip_19)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_20\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_21 + PWN(Clip_22)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_23 + PWN(Clip_24)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_25 + Add_26\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_27 + PWN(Clip_28)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_29 + PWN(Clip_30)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_31 + Add_32\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_33 + PWN(Clip_34)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_35 + PWN(Clip_36)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_37\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_38 + PWN(Clip_39)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_40 + PWN(Clip_41)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_42 + Add_43\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_44 + PWN(Clip_45)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_46 + PWN(Clip_47)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_48 + Add_49\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_50 + PWN(Clip_51)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_52 + PWN(Clip_53)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_54 + Add_55\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_56 + PWN(Clip_57)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_58 + PWN(Clip_59)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_61 + PWN(Clip_62)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_63 + PWN(Clip_64)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_65 + Add_66\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_67 + PWN(Clip_68)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_69 + PWN(Clip_70)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_71 + Add_72\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_73 + PWN(Clip_74)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_75 + PWN(Clip_76)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_77\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_78 + PWN(Clip_79)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_80 + PWN(Clip_81)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_82 + Add_83\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_84 + PWN(Clip_85)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_86 + PWN(Clip_87)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_88 + Add_89\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_90 + PWN(Clip_91)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_92 + PWN(Clip_93)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_94\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Conv_95 + PWN(Clip_96)\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] GlobalAveragePool_97\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] Gemm_99\n",
      "[09/22/2022-21:36:46] [I] [TRT] [GpuLayer] (Unnamed Layer* 114) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:36:47] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +145, now: CPU 450, GPU 3092 (MiB)\n",
      "[09/22/2022-21:36:48] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +249, now: CPU 690, GPU 3341 (MiB)\n",
      "[09/22/2022-21:36:48] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:37:20] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:40:53] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:40:53] [I] [TRT] Total Host Persistent Memory: 110832\n",
      "[09/22/2022-21:40:53] [I] [TRT] Total Device Persistent Memory: 4304896\n",
      "[09/22/2022-21:40:53] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 32 MiB\n",
      "[09/22/2022-21:40:53] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 8.94745ms to assign 4 blocks to 68 nodes requiring 7024640 bytes.\n",
      "[09/22/2022-21:40:53] [I] [TRT] Total Activation Memory: 7024640\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 943, GPU 3593 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 943, GPU 3593 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +6, GPU +8, now: CPU 6, GPU 8 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 941, GPU 3598 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] Loaded engine size: 7 MiB\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 941, GPU 3598 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +1, GPU +0, now: CPU 942, GPU 3598 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +7, now: CPU 0, GPU 7 (MiB)\n",
      "[09/22/2022-21:40:53] [I] Engine built in 249.492 sec.\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 898, GPU 3606 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 898, GPU 3606 (MiB)\n",
      "[09/22/2022-21:40:53] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +11, now: CPU 0, GPU 18 (MiB)\n",
      "[09/22/2022-21:40:53] [I] Using random values for input input__0\n",
      "[09/22/2022-21:40:53] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:40:53] [I] Using random values for output output__0\n",
      "[09/22/2022-21:40:53] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:40:53] [I] Starting inference\n",
      "[09/22/2022-21:40:56] [I] Warmup completed 16 queries over 200 ms\n",
      "[09/22/2022-21:40:56] [I] Timing trace has 245 queries over 3.02321 s\n",
      "[09/22/2022-21:40:56] [I] \n",
      "[09/22/2022-21:40:56] [I] === Trace details ===\n",
      "[09/22/2022-21:40:56] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.211 ms - Host latency: 12.2711 ms (end to end 12.2807 ms, enqueue 3.66977 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2575 ms - Host latency: 12.3173 ms (end to end 12.3269 ms, enqueue 4.07239 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1889 ms - Host latency: 12.2481 ms (end to end 12.2578 ms, enqueue 3.38729 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1934 ms - Host latency: 12.2527 ms (end to end 12.2622 ms, enqueue 3.30041 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2197 ms - Host latency: 12.279 ms (end to end 12.2887 ms, enqueue 3.32477 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.5138 ms - Host latency: 12.5735 ms (end to end 12.5829 ms, enqueue 3.35925 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2499 ms - Host latency: 12.3097 ms (end to end 12.3193 ms, enqueue 3.69951 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2436 ms - Host latency: 12.304 ms (end to end 12.3136 ms, enqueue 3.95105 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.5542 ms - Host latency: 12.613 ms (end to end 12.6226 ms, enqueue 3.24449 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2116 ms - Host latency: 12.2707 ms (end to end 12.2802 ms, enqueue 3.31741 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1848 ms - Host latency: 12.2436 ms (end to end 12.2532 ms, enqueue 3.1739 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1971 ms - Host latency: 12.2563 ms (end to end 12.2657 ms, enqueue 3.18163 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2344 ms - Host latency: 12.2941 ms (end to end 12.3038 ms, enqueue 3.99564 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2178 ms - Host latency: 12.2776 ms (end to end 12.2872 ms, enqueue 3.55016 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1965 ms - Host latency: 12.2554 ms (end to end 12.265 ms, enqueue 3.20078 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.5211 ms - Host latency: 12.5806 ms (end to end 12.5903 ms, enqueue 3.4535 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2032 ms - Host latency: 12.2623 ms (end to end 12.2719 ms, enqueue 3.23313 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2224 ms - Host latency: 12.2819 ms (end to end 12.2916 ms, enqueue 3.4405 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.6036 ms - Host latency: 12.6643 ms (end to end 12.6837 ms, enqueue 4.00559 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.196 ms - Host latency: 12.2551 ms (end to end 12.2648 ms, enqueue 3.41553 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.1892 ms - Host latency: 12.2482 ms (end to end 12.258 ms, enqueue 3.20122 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.217 ms - Host latency: 12.2762 ms (end to end 12.286 ms, enqueue 3.32461 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2066 ms - Host latency: 12.2657 ms (end to end 12.2755 ms, enqueue 3.21958 ms)\n",
      "[09/22/2022-21:40:56] [I] Average on 10 runs - GPU latency: 12.2304 ms - Host latency: 12.2901 ms (end to end 12.3001 ms, enqueue 3.85989 ms)\n",
      "[09/22/2022-21:40:56] [I] \n",
      "[09/22/2022-21:40:56] [I] === Performance summary ===\n",
      "[09/22/2022-21:40:56] [I] Throughput: 81.0398 qps\n",
      "[09/22/2022-21:40:56] [I] Latency: min = 12.1864 ms, max = 15.8944 ms, mean = 12.329 ms, median = 12.269 ms, percentile(99%) = 15.4077 ms\n",
      "[09/22/2022-21:40:56] [I] End-to-End Host Latency: min = 12.1956 ms, max = 15.9045 ms, mean = 12.339 ms, median = 12.2786 ms, percentile(99%) = 15.4177 ms\n",
      "[09/22/2022-21:40:56] [I] Enqueue Time: min = 2.91327 ms, max = 5.60205 ms, mean = 3.49195 ms, median = 3.34473 ms, percentile(99%) = 4.84387 ms\n",
      "[09/22/2022-21:40:56] [I] H2D Latency: min = 0.0548401 ms, max = 0.0622559 ms, mean = 0.0566284 ms, median = 0.0563354 ms, percentile(99%) = 0.0600586 ms\n",
      "[09/22/2022-21:40:56] [I] GPU Compute Time: min = 12.1273 ms, max = 15.8357 ms, mean = 12.2695 ms, median = 12.21 ms, percentile(99%) = 15.3489 ms\n",
      "[09/22/2022-21:40:56] [I] D2H Latency: min = 0.00170898 ms, max = 0.00341797 ms, mean = 0.00283957 ms, median = 0.00286865 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-21:40:56] [I] Total Host Walltime: 3.02321 s\n",
      "[09/22/2022-21:40:56] [I] Total GPU Compute Time: 3.00604 s\n",
      "[09/22/2022-21:40:56] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:40:56] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/mobilenet_v2-trt_fp16.plan\n",
      "./Flask/Models/mobilenet_v2-trt_fp16.plan\n",
      "./Triton/Models/mobilenet_v2-trt_fp16/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838b611a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/mobilenet_v2-trt_int8.plan\n",
      "[09/22/2022-21:40:57] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:40:57] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:40:57] [I] === Model Options ===\n",
      "[09/22/2022-21:40:57] [I] Format: ONNX\n",
      "[09/22/2022-21:40:57] [I] Model: ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:40:57] [I] Output:\n",
      "[09/22/2022-21:40:57] [I] === Build Options ===\n",
      "[09/22/2022-21:40:57] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:40:57] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:40:57] [I] minTiming: 1\n",
      "[09/22/2022-21:40:57] [I] avgTiming: 8\n",
      "[09/22/2022-21:40:57] [I] Precision: FP32+INT8\n",
      "[09/22/2022-21:40:57] [I] Calibration: Dynamic\n",
      "[09/22/2022-21:40:57] [I] Refit: Disabled\n",
      "[09/22/2022-21:40:57] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:40:57] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:40:57] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:40:57] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:40:57] [I] Save engine: ./Flask/Models/mobilenet_v2-trt_int8.plan\n",
      "[09/22/2022-21:40:57] [I] Load engine: \n",
      "[09/22/2022-21:40:57] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:40:57] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:40:57] [I] timingCacheMode: local\n",
      "[09/22/2022-21:40:57] [I] timingCacheFile: \n",
      "[09/22/2022-21:40:57] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:40:57] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:40:57] [I] Input build shapes: model\n",
      "[09/22/2022-21:40:57] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:40:57] [I] === System Options ===\n",
      "[09/22/2022-21:40:57] [I] Device: 0\n",
      "[09/22/2022-21:40:57] [I] DLACore: \n",
      "[09/22/2022-21:40:57] [I] Plugins:\n",
      "[09/22/2022-21:40:57] [I] === Inference Options ===\n",
      "[09/22/2022-21:40:57] [I] Batch: Explicit\n",
      "[09/22/2022-21:40:57] [I] Input inference shapes: model\n",
      "[09/22/2022-21:40:57] [I] Iterations: 10\n",
      "[09/22/2022-21:40:57] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:40:57] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:40:57] [I] Idle time: 0ms\n",
      "[09/22/2022-21:40:57] [I] Streams: 1\n",
      "[09/22/2022-21:40:57] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:40:57] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:40:57] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:40:57] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:40:57] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:40:57] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:40:57] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:40:57] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:40:57] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:40:57] [I] Inputs:\n",
      "[09/22/2022-21:40:57] [I] === Reporting Options ===\n",
      "[09/22/2022-21:40:57] [I] Verbose: Disabled\n",
      "[09/22/2022-21:40:57] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:40:57] [I] Percentile: 99\n",
      "[09/22/2022-21:40:57] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:40:57] [I] Dump output: Disabled\n",
      "[09/22/2022-21:40:57] [I] Profile: Disabled\n",
      "[09/22/2022-21:40:57] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:40:57] [I] Export output to JSON file: \n",
      "[09/22/2022-21:40:57] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:40:57] [I] \n",
      "[09/22/2022-21:40:57] [I] === Device Information ===\n",
      "[09/22/2022-21:40:57] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:40:57] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:40:57] [I] SMs: 1\n",
      "[09/22/2022-21:40:57] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:40:57] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:40:57] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:40:57] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:40:57] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:40:57] [I] \n",
      "[09/22/2022-21:40:57] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:40:59] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2890 (MiB)\n",
      "[09/22/2022-21:40:59] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2890 MiB\n",
      "[09/22/2022-21:41:00] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2918 MiB\n",
      "[09/22/2022-21:41:00] [I] Start parsing network model\n",
      "[09/22/2022-21:41:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:41:00] [I] [TRT] Input filename:   ./Flask/Models/mobilenet_v2-onnx.onnx\n",
      "[09/22/2022-21:41:00] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:41:00] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:41:00] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:41:00] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:41:00] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:41:00] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:41:00] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:41:00] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:41:00] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:41:00] [I] Finish parsing network model\n",
      "[09/22/2022-21:41:00] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:41:00] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best\n",
      "[09/22/2022-21:41:00] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-21:41:00] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-21:41:00] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:41:00] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_0 + PWN(Clip_1)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_2 + PWN(Clip_3)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_4\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_5 + PWN(Clip_6)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_7 + PWN(Clip_8)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_9\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_10 + PWN(Clip_11)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_12 + PWN(Clip_13)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_14 + Add_15\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_16 + PWN(Clip_17)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_18 + PWN(Clip_19)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_20\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_21 + PWN(Clip_22)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_23 + PWN(Clip_24)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_25 + Add_26\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_27 + PWN(Clip_28)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_29 + PWN(Clip_30)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_31 + Add_32\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_33 + PWN(Clip_34)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_35 + PWN(Clip_36)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_37\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_38 + PWN(Clip_39)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_40 + PWN(Clip_41)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_42 + Add_43\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_44 + PWN(Clip_45)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_46 + PWN(Clip_47)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_48 + Add_49\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_50 + PWN(Clip_51)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_52 + PWN(Clip_53)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_54 + Add_55\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_56 + PWN(Clip_57)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_58 + PWN(Clip_59)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_61 + PWN(Clip_62)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_63 + PWN(Clip_64)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_65 + Add_66\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_67 + PWN(Clip_68)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_69 + PWN(Clip_70)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_71 + Add_72\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_73 + PWN(Clip_74)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_75 + PWN(Clip_76)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_77\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_78 + PWN(Clip_79)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_80 + PWN(Clip_81)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_82 + Add_83\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_84 + PWN(Clip_85)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_86 + PWN(Clip_87)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_88 + Add_89\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_90 + PWN(Clip_91)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_92 + PWN(Clip_93)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_94\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Conv_95 + PWN(Clip_96)\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] GlobalAveragePool_97\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] Gemm_99\n",
      "[09/22/2022-21:41:00] [I] [TRT] [GpuLayer] (Unnamed Layer* 114) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:41:01] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +150, now: CPU 450, GPU 3097 (MiB)\n",
      "[09/22/2022-21:41:02] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +237, now: CPU 690, GPU 3334 (MiB)\n",
      "[09/22/2022-21:41:02] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:41:25] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:44:05] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:44:05] [I] [TRT] Total Host Persistent Memory: 107440\n",
      "[09/22/2022-21:44:05] [I] [TRT] Total Device Persistent Memory: 7301120\n",
      "[09/22/2022-21:44:05] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 8 MiB, GPU 32 MiB\n",
      "[09/22/2022-21:44:05] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 6.03249ms to assign 4 blocks to 56 nodes requiring 7024640 bytes.\n",
      "[09/22/2022-21:44:05] [I] [TRT] Total Activation Memory: 7024640\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 939, GPU 3662 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 939, GPU 3662 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +16, now: CPU 0, GPU 16 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 946, GPU 3670 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] Loaded engine size: 13 MiB\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 947, GPU 3670 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 947, GPU 3670 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +12, now: CPU 0, GPU 12 (MiB)\n",
      "[09/22/2022-21:44:05] [I] Engine built in 187.938 sec.\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +0, now: CPU 904, GPU 3683 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 904, GPU 3683 (MiB)\n",
      "[09/22/2022-21:44:05] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +14, now: CPU 0, GPU 26 (MiB)\n",
      "[09/22/2022-21:44:05] [I] Using random values for input input__0\n",
      "[09/22/2022-21:44:05] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:44:05] [I] Using random values for output output__0\n",
      "[09/22/2022-21:44:05] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:44:05] [I] Starting inference\n",
      "[09/22/2022-21:44:09] [I] Warmup completed 15 queries over 200 ms\n",
      "[09/22/2022-21:44:09] [I] Timing trace has 226 queries over 3.03485 s\n",
      "[09/22/2022-21:44:09] [I] \n",
      "[09/22/2022-21:44:09] [I] === Trace details ===\n",
      "[09/22/2022-21:44:09] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.281 ms - Host latency: 13.3405 ms (end to end 13.3509 ms, enqueue 2.78011 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2766 ms - Host latency: 13.3362 ms (end to end 13.3465 ms, enqueue 2.78055 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.5718 ms - Host latency: 13.6315 ms (end to end 13.6416 ms, enqueue 2.87363 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.3209 ms - Host latency: 13.3813 ms (end to end 13.3916 ms, enqueue 2.98108 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.3526 ms - Host latency: 13.4128 ms (end to end 13.4231 ms, enqueue 3.30421 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.5803 ms - Host latency: 13.6406 ms (end to end 13.651 ms, enqueue 3.13056 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2835 ms - Host latency: 13.3437 ms (end to end 13.3543 ms, enqueue 2.94073 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2736 ms - Host latency: 13.3332 ms (end to end 13.3433 ms, enqueue 2.93286 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.29 ms - Host latency: 13.35 ms (end to end 13.36 ms, enqueue 3.00043 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.3315 ms - Host latency: 13.3936 ms (end to end 13.4036 ms, enqueue 3.51321 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2688 ms - Host latency: 13.3289 ms (end to end 13.3394 ms, enqueue 2.84791 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.5752 ms - Host latency: 13.6356 ms (end to end 13.6459 ms, enqueue 2.81382 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2861 ms - Host latency: 13.3461 ms (end to end 13.3564 ms, enqueue 2.70714 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2628 ms - Host latency: 13.3228 ms (end to end 13.333 ms, enqueue 2.67825 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.6123 ms - Host latency: 13.6734 ms (end to end 13.6836 ms, enqueue 3.41006 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2917 ms - Host latency: 13.3522 ms (end to end 13.3627 ms, enqueue 2.91667 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2769 ms - Host latency: 13.3368 ms (end to end 13.347 ms, enqueue 2.82578 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2804 ms - Host latency: 13.3404 ms (end to end 13.3508 ms, enqueue 2.72478 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2783 ms - Host latency: 13.3379 ms (end to end 13.3485 ms, enqueue 2.6926 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.3333 ms - Host latency: 13.3942 ms (end to end 13.4047 ms, enqueue 3.18936 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.6122 ms - Host latency: 13.6728 ms (end to end 13.6834 ms, enqueue 3.2582 ms)\n",
      "[09/22/2022-21:44:09] [I] Average on 10 runs - GPU latency: 13.2745 ms - Host latency: 13.3345 ms (end to end 13.345 ms, enqueue 2.78032 ms)\n",
      "[09/22/2022-21:44:09] [I] \n",
      "[09/22/2022-21:44:09] [I] === Performance summary ===\n",
      "[09/22/2022-21:44:09] [I] Throughput: 74.4682 qps\n",
      "[09/22/2022-21:44:09] [I] Latency: min = 13.212 ms, max = 16.3479 ms, mean = 13.4176 ms, median = 13.3488 ms, percentile(99%) = 16.1925 ms\n",
      "[09/22/2022-21:44:09] [I] End-to-End Host Latency: min = 13.2225 ms, max = 16.3584 ms, mean = 13.428 ms, median = 13.3593 ms, percentile(99%) = 16.2023 ms\n",
      "[09/22/2022-21:44:09] [I] Enqueue Time: min = 2.39661 ms, max = 3.93506 ms, mean = 2.95829 ms, median = 2.88495 ms, percentile(99%) = 3.78491 ms\n",
      "[09/22/2022-21:44:09] [I] H2D Latency: min = 0.0549316 ms, max = 0.0618896 ms, mean = 0.0572616 ms, median = 0.0570221 ms, percentile(99%) = 0.060791 ms\n",
      "[09/22/2022-21:44:09] [I] GPU Compute Time: min = 13.1499 ms, max = 16.2883 ms, mean = 13.3574 ms, median = 13.2889 ms, percentile(99%) = 16.1324 ms\n",
      "[09/22/2022-21:44:09] [I] D2H Latency: min = 0.00146484 ms, max = 0.00366211 ms, mean = 0.00293239 ms, median = 0.00292969 ms, percentile(99%) = 0.00354004 ms\n",
      "[09/22/2022-21:44:09] [I] Total Host Walltime: 3.03485 s\n",
      "[09/22/2022-21:44:09] [I] Total GPU Compute Time: 3.01878 s\n",
      "[09/22/2022-21:44:09] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:44:09] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/mobilenet_v2-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/mobilenet_v2-trt_int8.plan\n",
      "./Flask/Models/mobilenet_v2-trt_int8.plan\n",
      "./Triton/Models/mobilenet_v2-trt_int8/config.pbtxt\n",
      "./Triton/Models/mobilenet_v2-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'mobilenet_v2-onnx'\n",
    "trt_model = 'mobilenet_v2-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b2e97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/efficientnet_b0-trt_best.plan\n",
      "[09/22/2022-21:44:10] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:44:10] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:44:10] [I] === Model Options ===\n",
      "[09/22/2022-21:44:10] [I] Format: ONNX\n",
      "[09/22/2022-21:44:10] [I] Model: ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-21:44:10] [I] Output:\n",
      "[09/22/2022-21:44:10] [I] === Build Options ===\n",
      "[09/22/2022-21:44:10] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:44:10] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:44:10] [I] minTiming: 1\n",
      "[09/22/2022-21:44:10] [I] avgTiming: 8\n",
      "[09/22/2022-21:44:10] [I] Precision: FP32+FP16+INT8\n",
      "[09/22/2022-21:44:10] [I] Calibration: Dynamic\n",
      "[09/22/2022-21:44:10] [I] Refit: Disabled\n",
      "[09/22/2022-21:44:10] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:44:10] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:44:10] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:44:10] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:44:10] [I] Save engine: ./Flask/Models/efficientnet_b0-trt_best.plan\n",
      "[09/22/2022-21:44:10] [I] Load engine: \n",
      "[09/22/2022-21:44:10] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:44:10] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:44:10] [I] timingCacheMode: local\n",
      "[09/22/2022-21:44:10] [I] timingCacheFile: \n",
      "[09/22/2022-21:44:10] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:44:10] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:44:10] [I] Input build shapes: model\n",
      "[09/22/2022-21:44:10] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:44:10] [I] === System Options ===\n",
      "[09/22/2022-21:44:10] [I] Device: 0\n",
      "[09/22/2022-21:44:10] [I] DLACore: \n",
      "[09/22/2022-21:44:10] [I] Plugins:\n",
      "[09/22/2022-21:44:10] [I] === Inference Options ===\n",
      "[09/22/2022-21:44:10] [I] Batch: Explicit\n",
      "[09/22/2022-21:44:10] [I] Input inference shapes: model\n",
      "[09/22/2022-21:44:10] [I] Iterations: 10\n",
      "[09/22/2022-21:44:10] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:44:10] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:44:10] [I] Idle time: 0ms\n",
      "[09/22/2022-21:44:10] [I] Streams: 1\n",
      "[09/22/2022-21:44:10] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:44:10] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:44:10] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:44:10] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:44:10] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:44:10] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:44:10] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:44:10] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:44:10] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:44:10] [I] Inputs:\n",
      "[09/22/2022-21:44:10] [I] === Reporting Options ===\n",
      "[09/22/2022-21:44:10] [I] Verbose: Disabled\n",
      "[09/22/2022-21:44:10] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:44:10] [I] Percentile: 99\n",
      "[09/22/2022-21:44:10] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:44:10] [I] Dump output: Disabled\n",
      "[09/22/2022-21:44:10] [I] Profile: Disabled\n",
      "[09/22/2022-21:44:10] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:44:10] [I] Export output to JSON file: \n",
      "[09/22/2022-21:44:10] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:44:10] [I] \n",
      "[09/22/2022-21:44:10] [I] === Device Information ===\n",
      "[09/22/2022-21:44:10] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:44:10] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:44:10] [I] SMs: 1\n",
      "[09/22/2022-21:44:10] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:44:10] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:44:10] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:44:10] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:44:10] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:44:10] [I] \n",
      "[09/22/2022-21:44:10] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:44:11] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2971 (MiB)\n",
      "[09/22/2022-21:44:12] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2971 MiB\n",
      "[09/22/2022-21:44:12] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 2999 MiB\n",
      "[09/22/2022-21:44:12] [I] Start parsing network model\n",
      "[09/22/2022-21:44:12] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:44:12] [I] [TRT] Input filename:   ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-21:44:12] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:44:12] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:44:12] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:44:12] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:44:12] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:44:12] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:44:12] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:44:12] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:44:12] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:44:13] [I] Finish parsing network model\n",
      "[09/22/2022-21:44:13] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:44:13] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-21:44:13] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-21:44:13] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:44:13] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_0\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_1), Mul_2)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_3\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_4), Mul_5)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_6\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_7\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_8), Mul_9)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_10\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_11), Mul_12)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_13\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_14\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_15), Mul_16)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_17\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_18), Mul_19)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_20\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_22), Mul_23)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_24\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_25), Mul_26)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_27\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_28\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_29), Mul_30)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_31\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_32), Mul_33)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_34\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_35\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_36), Mul_37)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_38\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_39), Mul_40)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_41 + Add_42\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_43\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_44), Mul_45)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_46\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_47), Mul_48)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_49\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_50\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_51), Mul_52)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_53\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_54), Mul_55)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_56\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_57\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_58), Mul_59)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_61), Mul_62)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_63\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_64\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_65), Mul_66)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_67\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_68), Mul_69)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_70 + Add_71\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_72\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_73), Mul_74)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_75\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_76), Mul_77)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_78\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_79\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_80), Mul_81)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_82\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_83), Mul_84)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_85\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_86\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_87), Mul_88)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_89\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_90), Mul_91)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_92\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_93\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_94), Mul_95)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_96\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_97), Mul_98)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_99 + Add_100\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_101\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_102), Mul_103)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_104\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_105), Mul_106)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_107\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_108\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_109), Mul_110)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_111\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_112), Mul_113)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_114 + Add_115\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_116\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_117), Mul_118)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_119\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_120), Mul_121)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_122\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_123\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_124), Mul_125)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_126\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_127), Mul_128)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_129\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_130\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_131), Mul_132)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_133\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_134), Mul_135)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_136\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_137\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_138), Mul_139)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_140\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_141), Mul_142)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_143 + Add_144\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_145\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_146), Mul_147)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_148\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_149), Mul_150)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_151\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_152\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_153), Mul_154)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_155\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_156), Mul_157)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_158 + Add_159\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_160\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_161), Mul_162)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_163\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_164), Mul_165)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_166\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_167\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_168), Mul_169)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_170\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_171), Mul_172)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_173\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_174\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_175), Mul_176)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_177\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_178), Mul_179)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_180\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_181\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_182), Mul_183)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_184\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_185), Mul_186)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_187 + Add_188\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_189\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_190), Mul_191)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_192\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_193), Mul_194)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_195\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_196\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_197), Mul_198)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_199\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_200), Mul_201)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_202 + Add_203\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_204\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_205), Mul_206)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_207\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_208), Mul_209)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_210\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_211\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_212), Mul_213)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_214\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_215), Mul_216)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_217 + Add_218\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_219\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_220), Mul_221)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_222\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_223), Mul_224)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_225\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_226\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_227), Mul_228)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_229\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_230), Mul_231)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_232\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Conv_233\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_234), Mul_235)\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] GlobalAveragePool_236\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] Gemm_238\n",
      "[09/22/2022-21:44:13] [I] [TRT] [GpuLayer] (Unnamed Layer* 253) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:44:14] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +139, now: CPU 457, GPU 3201 (MiB)\n",
      "[09/22/2022-21:44:15] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +250, now: CPU 698, GPU 3451 (MiB)\n",
      "[09/22/2022-21:44:15] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:48:41] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-21:55:19] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-21:55:20] [I] [TRT] Total Host Persistent Memory: 157264\n",
      "[09/22/2022-21:55:20] [I] [TRT] Total Device Persistent Memory: 5713920\n",
      "[09/22/2022-21:55:20] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 12 MiB, GPU 51 MiB\n",
      "[09/22/2022-21:55:20] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 137.896ms to assign 5 blocks to 217 nodes requiring 10016768 bytes.\n",
      "[09/22/2022-21:55:20] [I] [TRT] Total Activation Memory: 10016768\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +0, now: CPU 974, GPU 3695 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 974, GPU 3695 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +10, GPU +16, now: CPU 10, GPU 16 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 956, GPU 3698 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] Loaded engine size: 11 MiB\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 965, GPU 3697 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 965, GPU 3697 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +10, now: CPU 0, GPU 10 (MiB)\n",
      "[09/22/2022-21:55:20] [I] Engine built in 670.721 sec.\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 919, GPU 3710 (MiB)\n",
      "[09/22/2022-21:55:20] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 919, GPU 3710 (MiB)\n",
      "[09/22/2022-21:55:21] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +15, now: CPU 0, GPU 25 (MiB)\n",
      "[09/22/2022-21:55:21] [I] Using random values for input input__0\n",
      "[09/22/2022-21:55:21] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-21:55:21] [I] Using random values for output output__0\n",
      "[09/22/2022-21:55:21] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-21:55:21] [I] Starting inference\n",
      "[09/22/2022-21:55:24] [I] Warmup completed 9 queries over 200 ms\n",
      "[09/22/2022-21:55:24] [I] Timing trace has 140 queries over 3.05976 s\n",
      "[09/22/2022-21:55:24] [I] \n",
      "[09/22/2022-21:55:24] [I] === Trace details ===\n",
      "[09/22/2022-21:55:24] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.7154 ms - Host latency: 21.7757 ms (end to end 21.7852 ms, enqueue 10.7534 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6305 ms - Host latency: 21.6894 ms (end to end 21.6986 ms, enqueue 10.3159 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6313 ms - Host latency: 21.69 ms (end to end 21.6993 ms, enqueue 10.7464 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 22.0774 ms - Host latency: 22.1371 ms (end to end 22.1465 ms, enqueue 10.6183 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.7003 ms - Host latency: 21.7594 ms (end to end 21.7689 ms, enqueue 9.64106 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 22.0002 ms - Host latency: 22.0597 ms (end to end 22.0694 ms, enqueue 9.02706 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6916 ms - Host latency: 21.7506 ms (end to end 21.7601 ms, enqueue 10.049 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.7107 ms - Host latency: 21.7704 ms (end to end 21.78 ms, enqueue 10.2526 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 22.0388 ms - Host latency: 22.0979 ms (end to end 22.1074 ms, enqueue 9.05763 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.7018 ms - Host latency: 21.7609 ms (end to end 21.7703 ms, enqueue 9.17285 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 22.0447 ms - Host latency: 22.1049 ms (end to end 22.1145 ms, enqueue 10.8422 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6925 ms - Host latency: 21.7511 ms (end to end 21.7606 ms, enqueue 8.94495 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6992 ms - Host latency: 21.758 ms (end to end 21.7678 ms, enqueue 9.05088 ms)\n",
      "[09/22/2022-21:55:24] [I] Average on 10 runs - GPU latency: 21.6703 ms - Host latency: 21.73 ms (end to end 21.7392 ms, enqueue 12.6735 ms)\n",
      "[09/22/2022-21:55:24] [I] \n",
      "[09/22/2022-21:55:24] [I] === Performance summary ===\n",
      "[09/22/2022-21:55:24] [I] Throughput: 45.7552 qps\n",
      "[09/22/2022-21:55:24] [I] Latency: min = 21.6179 ms, max = 25.3259 ms, mean = 21.8454 ms, median = 21.7465 ms, percentile(99%) = 25.2936 ms\n",
      "[09/22/2022-21:55:24] [I] End-to-End Host Latency: min = 21.6277 ms, max = 25.3352 ms, mean = 21.8548 ms, median = 21.7563 ms, percentile(99%) = 25.3028 ms\n",
      "[09/22/2022-21:55:24] [I] Enqueue Time: min = 8.5094 ms, max = 14.8049 ms, mean = 10.0818 ms, median = 9.69122 ms, percentile(99%) = 14.8042 ms\n",
      "[09/22/2022-21:55:24] [I] H2D Latency: min = 0.0549316 ms, max = 0.0598145 ms, mean = 0.0565063 ms, median = 0.0561523 ms, percentile(99%) = 0.0597534 ms\n",
      "[09/22/2022-21:55:24] [I] GPU Compute Time: min = 21.5596 ms, max = 25.2673 ms, mean = 21.786 ms, median = 21.6876 ms, percentile(99%) = 25.2347 ms\n",
      "[09/22/2022-21:55:24] [I] D2H Latency: min = 0.00170898 ms, max = 0.00341797 ms, mean = 0.00283399 ms, median = 0.00280762 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-21:55:24] [I] Total Host Walltime: 3.05976 s\n",
      "[09/22/2022-21:55:24] [I] Total GPU Compute Time: 3.05005 s\n",
      "[09/22/2022-21:55:24] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-21:55:24] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --best --saveEngine=./Flask/Models/efficientnet_b0-trt_best.plan\n",
      "./Flask/Models/efficientnet_b0-trt_best.plan\n",
      "./Triton/Models/efficientnet_b0-trt_best/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_best/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'best'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef3fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/efficientnet_b0-trt_fp16.plan\n",
      "[09/22/2022-21:55:25] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-21:55:25] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-21:55:25] [I] === Model Options ===\n",
      "[09/22/2022-21:55:25] [I] Format: ONNX\n",
      "[09/22/2022-21:55:25] [I] Model: ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-21:55:25] [I] Output:\n",
      "[09/22/2022-21:55:25] [I] === Build Options ===\n",
      "[09/22/2022-21:55:25] [I] Max batch: explicit batch\n",
      "[09/22/2022-21:55:25] [I] Workspace: 16 MiB\n",
      "[09/22/2022-21:55:25] [I] minTiming: 1\n",
      "[09/22/2022-21:55:25] [I] avgTiming: 8\n",
      "[09/22/2022-21:55:25] [I] Precision: FP32+FP16\n",
      "[09/22/2022-21:55:25] [I] Calibration: \n",
      "[09/22/2022-21:55:25] [I] Refit: Disabled\n",
      "[09/22/2022-21:55:25] [I] Sparsity: Disabled\n",
      "[09/22/2022-21:55:25] [I] Safe mode: Disabled\n",
      "[09/22/2022-21:55:25] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-21:55:25] [I] Restricted mode: Disabled\n",
      "[09/22/2022-21:55:25] [I] Save engine: ./Flask/Models/efficientnet_b0-trt_fp16.plan\n",
      "[09/22/2022-21:55:25] [I] Load engine: \n",
      "[09/22/2022-21:55:25] [I] Profiling verbosity: 0\n",
      "[09/22/2022-21:55:25] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-21:55:25] [I] timingCacheMode: local\n",
      "[09/22/2022-21:55:25] [I] timingCacheFile: \n",
      "[09/22/2022-21:55:25] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-21:55:25] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-21:55:25] [I] Input build shapes: model\n",
      "[09/22/2022-21:55:25] [I] Input calibration shapes: model\n",
      "[09/22/2022-21:55:25] [I] === System Options ===\n",
      "[09/22/2022-21:55:25] [I] Device: 0\n",
      "[09/22/2022-21:55:25] [I] DLACore: \n",
      "[09/22/2022-21:55:25] [I] Plugins:\n",
      "[09/22/2022-21:55:25] [I] === Inference Options ===\n",
      "[09/22/2022-21:55:25] [I] Batch: Explicit\n",
      "[09/22/2022-21:55:25] [I] Input inference shapes: model\n",
      "[09/22/2022-21:55:25] [I] Iterations: 10\n",
      "[09/22/2022-21:55:25] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-21:55:25] [I] Sleep time: 0ms\n",
      "[09/22/2022-21:55:25] [I] Idle time: 0ms\n",
      "[09/22/2022-21:55:25] [I] Streams: 1\n",
      "[09/22/2022-21:55:25] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-21:55:25] [I] Data transfers: Enabled\n",
      "[09/22/2022-21:55:25] [I] Spin-wait: Disabled\n",
      "[09/22/2022-21:55:25] [I] Multithreading: Disabled\n",
      "[09/22/2022-21:55:25] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-21:55:25] [I] Separate profiling: Disabled\n",
      "[09/22/2022-21:55:25] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-21:55:25] [I] Time Refit: Disabled\n",
      "[09/22/2022-21:55:25] [I] Skip inference: Disabled\n",
      "[09/22/2022-21:55:25] [I] Inputs:\n",
      "[09/22/2022-21:55:25] [I] === Reporting Options ===\n",
      "[09/22/2022-21:55:25] [I] Verbose: Disabled\n",
      "[09/22/2022-21:55:25] [I] Averages: 10 inferences\n",
      "[09/22/2022-21:55:25] [I] Percentile: 99\n",
      "[09/22/2022-21:55:25] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-21:55:25] [I] Dump output: Disabled\n",
      "[09/22/2022-21:55:25] [I] Profile: Disabled\n",
      "[09/22/2022-21:55:25] [I] Export timing to JSON file: \n",
      "[09/22/2022-21:55:25] [I] Export output to JSON file: \n",
      "[09/22/2022-21:55:25] [I] Export profile to JSON file: \n",
      "[09/22/2022-21:55:25] [I] \n",
      "[09/22/2022-21:55:25] [I] === Device Information ===\n",
      "[09/22/2022-21:55:25] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-21:55:25] [I] Compute Capability: 5.3\n",
      "[09/22/2022-21:55:25] [I] SMs: 1\n",
      "[09/22/2022-21:55:25] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-21:55:25] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-21:55:25] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-21:55:25] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-21:55:25] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-21:55:25] [I] \n",
      "[09/22/2022-21:55:25] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-21:55:27] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2989 (MiB)\n",
      "[09/22/2022-21:55:27] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2988 MiB\n",
      "[09/22/2022-21:55:27] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 3018 MiB\n",
      "[09/22/2022-21:55:27] [I] Start parsing network model\n",
      "[09/22/2022-21:55:27] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:55:27] [I] [TRT] Input filename:   ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-21:55:27] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-21:55:27] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-21:55:27] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-21:55:27] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-21:55:27] [I] [TRT] Domain:           \n",
      "[09/22/2022-21:55:27] [I] [TRT] Model version:    0\n",
      "[09/22/2022-21:55:27] [I] [TRT] Doc string:       \n",
      "[09/22/2022-21:55:27] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-21:55:27] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-21:55:27] [I] Finish parsing network model\n",
      "[09/22/2022-21:55:27] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-21:55:28] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-21:55:28] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_0\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_1), Mul_2)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_3\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_4), Mul_5)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_6\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_7\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_8), Mul_9)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_10\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_11), Mul_12)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_13\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_14\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_15), Mul_16)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_17\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_18), Mul_19)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_20\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_22), Mul_23)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_24\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_25), Mul_26)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_27\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_28\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_29), Mul_30)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_31\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_32), Mul_33)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_34\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_35\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_36), Mul_37)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_38\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_39), Mul_40)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_41 + Add_42\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_43\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_44), Mul_45)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_46\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_47), Mul_48)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_49\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_50\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_51), Mul_52)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_53\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_54), Mul_55)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_56\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_57\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_58), Mul_59)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_61), Mul_62)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_63\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_64\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_65), Mul_66)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_67\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_68), Mul_69)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_70 + Add_71\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_72\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_73), Mul_74)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_75\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_76), Mul_77)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_78\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_79\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_80), Mul_81)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_82\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_83), Mul_84)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_85\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_86\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_87), Mul_88)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_89\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_90), Mul_91)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_92\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_93\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_94), Mul_95)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_96\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_97), Mul_98)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_99 + Add_100\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_101\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_102), Mul_103)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_104\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_105), Mul_106)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_107\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_108\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_109), Mul_110)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_111\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_112), Mul_113)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_114 + Add_115\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_116\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_117), Mul_118)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_119\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_120), Mul_121)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_122\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_123\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_124), Mul_125)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_126\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_127), Mul_128)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_129\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_130\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_131), Mul_132)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_133\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_134), Mul_135)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_136\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_137\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_138), Mul_139)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_140\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_141), Mul_142)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_143 + Add_144\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_145\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_146), Mul_147)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_148\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_149), Mul_150)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_151\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_152\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_153), Mul_154)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_155\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_156), Mul_157)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_158 + Add_159\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_160\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_161), Mul_162)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_163\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_164), Mul_165)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_166\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_167\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_168), Mul_169)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_170\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_171), Mul_172)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_173\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_174\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_175), Mul_176)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_177\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_178), Mul_179)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_180\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_181\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_182), Mul_183)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_184\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_185), Mul_186)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_187 + Add_188\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_189\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_190), Mul_191)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_192\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_193), Mul_194)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_195\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_196\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_197), Mul_198)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_199\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_200), Mul_201)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_202 + Add_203\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_204\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_205), Mul_206)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_207\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_208), Mul_209)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_210\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_211\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_212), Mul_213)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_214\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_215), Mul_216)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_217 + Add_218\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_219\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_220), Mul_221)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_222\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_223), Mul_224)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_225\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_226\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_227), Mul_228)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_229\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_230), Mul_231)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_232\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Conv_233\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_234), Mul_235)\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] GlobalAveragePool_236\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] Gemm_238\n",
      "[09/22/2022-21:55:28] [I] [TRT] [GpuLayer] (Unnamed Layer* 253) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-21:55:29] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +158, GPU +139, now: CPU 457, GPU 3200 (MiB)\n",
      "[09/22/2022-21:55:30] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +241, GPU +247, now: CPU 698, GPU 3447 (MiB)\n",
      "[09/22/2022-21:55:30] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-21:58:02] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-22:03:36] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-22:03:37] [I] [TRT] Total Host Persistent Memory: 157184\n",
      "[09/22/2022-22:03:37] [I] [TRT] Total Device Persistent Memory: 5713920\n",
      "[09/22/2022-22:03:37] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 12 MiB, GPU 51 MiB\n",
      "[09/22/2022-22:03:37] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 144.466ms to assign 5 blocks to 218 nodes requiring 10016768 bytes.\n",
      "[09/22/2022-22:03:37] [I] [TRT] Total Activation Memory: 10016768\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 971, GPU 3683 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 971, GPU 3683 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +10, GPU +16, now: CPU 10, GPU 16 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 954, GPU 3686 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] Loaded engine size: 11 MiB\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 963, GPU 3685 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 963, GPU 3685 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +10, now: CPU 0, GPU 10 (MiB)\n",
      "[09/22/2022-22:03:37] [I] Engine built in 492.236 sec.\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 917, GPU 3697 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 917, GPU 3697 (MiB)\n",
      "[09/22/2022-22:03:37] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +15, now: CPU 0, GPU 25 (MiB)\n",
      "[09/22/2022-22:03:37] [I] Using random values for input input__0\n",
      "[09/22/2022-22:03:37] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-22:03:37] [I] Using random values for output output__0\n",
      "[09/22/2022-22:03:37] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-22:03:37] [I] Starting inference\n",
      "[09/22/2022-22:03:41] [I] Warmup completed 9 queries over 200 ms\n",
      "[09/22/2022-22:03:41] [I] Timing trace has 140 queries over 3.063 s\n",
      "[09/22/2022-22:03:41] [I] \n",
      "[09/22/2022-22:03:41] [I] === Trace details ===\n",
      "[09/22/2022-22:03:41] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6791 ms - Host latency: 21.7391 ms (end to end 21.7486 ms, enqueue 11.417 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6587 ms - Host latency: 21.7178 ms (end to end 21.7275 ms, enqueue 8.91754 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.9886 ms - Host latency: 22.0482 ms (end to end 22.0581 ms, enqueue 9.12067 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6742 ms - Host latency: 21.7339 ms (end to end 21.7438 ms, enqueue 10.7478 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 22.0333 ms - Host latency: 22.0922 ms (end to end 22.1016 ms, enqueue 10.2687 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6942 ms - Host latency: 21.7534 ms (end to end 21.763 ms, enqueue 9.19941 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.727 ms - Host latency: 21.7873 ms (end to end 21.7969 ms, enqueue 9.62616 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.7051 ms - Host latency: 21.7647 ms (end to end 21.7741 ms, enqueue 9.74569 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.7148 ms - Host latency: 21.774 ms (end to end 22.0545 ms, enqueue 8.94135 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 22.1 ms - Host latency: 22.1603 ms (end to end 22.1701 ms, enqueue 9.4104 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6676 ms - Host latency: 21.7279 ms (end to end 21.7378 ms, enqueue 11.3059 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6744 ms - Host latency: 21.7334 ms (end to end 21.7431 ms, enqueue 8.8344 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 21.6826 ms - Host latency: 21.7421 ms (end to end 21.7517 ms, enqueue 8.85154 ms)\n",
      "[09/22/2022-22:03:41] [I] Average on 10 runs - GPU latency: 22.0521 ms - Host latency: 22.1122 ms (end to end 22.1215 ms, enqueue 11.0046 ms)\n",
      "[09/22/2022-22:03:41] [I] \n",
      "[09/22/2022-22:03:41] [I] === Performance summary ===\n",
      "[09/22/2022-22:03:41] [I] Throughput: 45.7067 qps\n",
      "[09/22/2022-22:03:41] [I] Latency: min = 21.6054 ms, max = 25.6335 ms, mean = 21.849 ms, median = 21.748 ms, percentile(99%) = 25.3314 ms\n",
      "[09/22/2022-22:03:41] [I] End-to-End Host Latency: min = 21.6146 ms, max = 25.6428 ms, mean = 21.878 ms, median = 21.7589 ms, percentile(99%) = 25.3405 ms\n",
      "[09/22/2022-22:03:41] [I] Enqueue Time: min = 8.32739 ms, max = 14.8965 ms, mean = 9.81365 ms, median = 9.15596 ms, percentile(99%) = 14.6007 ms\n",
      "[09/22/2022-22:03:41] [I] H2D Latency: min = 0.0554199 ms, max = 0.0610352 ms, mean = 0.0567622 ms, median = 0.0564575 ms, percentile(99%) = 0.0596313 ms\n",
      "[09/22/2022-22:03:41] [I] GPU Compute Time: min = 21.546 ms, max = 25.5725 ms, mean = 21.7894 ms, median = 21.6887 ms, percentile(99%) = 25.2727 ms\n",
      "[09/22/2022-22:03:41] [I] D2H Latency: min = 0.00170898 ms, max = 0.00341797 ms, mean = 0.00287846 ms, median = 0.00292969 ms, percentile(99%) = 0.0032959 ms\n",
      "[09/22/2022-22:03:41] [I] Total Host Walltime: 3.063 s\n",
      "[09/22/2022-22:03:41] [I] Total GPU Compute Time: 3.05052 s\n",
      "[09/22/2022-22:03:41] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-22:03:41] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --fp16 --saveEngine=./Flask/Models/efficientnet_b0-trt_fp16.plan\n",
      "./Flask/Models/efficientnet_b0-trt_fp16.plan\n",
      "./Triton/Models/efficientnet_b0-trt_fp16/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_fp16/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'fp16'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eee83847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&&&& RUNNING TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/efficientnet_b0-trt_int8.plan\n",
      "[09/22/2022-22:03:42] [W] --explicitBatch flag has been deprecated and has no effect!\n",
      "[09/22/2022-22:03:42] [W] Explicit batch dim is automatically enabled if input model is ONNX or if dynamic shapes are provided when the engine is built.\n",
      "[09/22/2022-22:03:42] [I] === Model Options ===\n",
      "[09/22/2022-22:03:42] [I] Format: ONNX\n",
      "[09/22/2022-22:03:42] [I] Model: ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-22:03:42] [I] Output:\n",
      "[09/22/2022-22:03:42] [I] === Build Options ===\n",
      "[09/22/2022-22:03:42] [I] Max batch: explicit batch\n",
      "[09/22/2022-22:03:42] [I] Workspace: 16 MiB\n",
      "[09/22/2022-22:03:42] [I] minTiming: 1\n",
      "[09/22/2022-22:03:42] [I] avgTiming: 8\n",
      "[09/22/2022-22:03:42] [I] Precision: FP32+INT8\n",
      "[09/22/2022-22:03:42] [I] Calibration: Dynamic\n",
      "[09/22/2022-22:03:42] [I] Refit: Disabled\n",
      "[09/22/2022-22:03:42] [I] Sparsity: Disabled\n",
      "[09/22/2022-22:03:42] [I] Safe mode: Disabled\n",
      "[09/22/2022-22:03:42] [I] DirectIO mode: Disabled\n",
      "[09/22/2022-22:03:42] [I] Restricted mode: Disabled\n",
      "[09/22/2022-22:03:42] [I] Save engine: ./Flask/Models/efficientnet_b0-trt_int8.plan\n",
      "[09/22/2022-22:03:42] [I] Load engine: \n",
      "[09/22/2022-22:03:42] [I] Profiling verbosity: 0\n",
      "[09/22/2022-22:03:42] [I] Tactic sources: Using default tactic sources\n",
      "[09/22/2022-22:03:42] [I] timingCacheMode: local\n",
      "[09/22/2022-22:03:42] [I] timingCacheFile: \n",
      "[09/22/2022-22:03:42] [I] Input(s)s format: fp32:CHW\n",
      "[09/22/2022-22:03:42] [I] Output(s)s format: fp32:CHW\n",
      "[09/22/2022-22:03:42] [I] Input build shapes: model\n",
      "[09/22/2022-22:03:42] [I] Input calibration shapes: model\n",
      "[09/22/2022-22:03:42] [I] === System Options ===\n",
      "[09/22/2022-22:03:42] [I] Device: 0\n",
      "[09/22/2022-22:03:42] [I] DLACore: \n",
      "[09/22/2022-22:03:42] [I] Plugins:\n",
      "[09/22/2022-22:03:42] [I] === Inference Options ===\n",
      "[09/22/2022-22:03:42] [I] Batch: Explicit\n",
      "[09/22/2022-22:03:42] [I] Input inference shapes: model\n",
      "[09/22/2022-22:03:42] [I] Iterations: 10\n",
      "[09/22/2022-22:03:42] [I] Duration: 3s (+ 200ms warm up)\n",
      "[09/22/2022-22:03:42] [I] Sleep time: 0ms\n",
      "[09/22/2022-22:03:42] [I] Idle time: 0ms\n",
      "[09/22/2022-22:03:42] [I] Streams: 1\n",
      "[09/22/2022-22:03:42] [I] ExposeDMA: Disabled\n",
      "[09/22/2022-22:03:42] [I] Data transfers: Enabled\n",
      "[09/22/2022-22:03:42] [I] Spin-wait: Disabled\n",
      "[09/22/2022-22:03:42] [I] Multithreading: Disabled\n",
      "[09/22/2022-22:03:42] [I] CUDA Graph: Disabled\n",
      "[09/22/2022-22:03:42] [I] Separate profiling: Disabled\n",
      "[09/22/2022-22:03:42] [I] Time Deserialize: Disabled\n",
      "[09/22/2022-22:03:42] [I] Time Refit: Disabled\n",
      "[09/22/2022-22:03:42] [I] Skip inference: Disabled\n",
      "[09/22/2022-22:03:42] [I] Inputs:\n",
      "[09/22/2022-22:03:42] [I] === Reporting Options ===\n",
      "[09/22/2022-22:03:42] [I] Verbose: Disabled\n",
      "[09/22/2022-22:03:42] [I] Averages: 10 inferences\n",
      "[09/22/2022-22:03:42] [I] Percentile: 99\n",
      "[09/22/2022-22:03:42] [I] Dump refittable layers:Disabled\n",
      "[09/22/2022-22:03:42] [I] Dump output: Disabled\n",
      "[09/22/2022-22:03:42] [I] Profile: Disabled\n",
      "[09/22/2022-22:03:42] [I] Export timing to JSON file: \n",
      "[09/22/2022-22:03:42] [I] Export output to JSON file: \n",
      "[09/22/2022-22:03:42] [I] Export profile to JSON file: \n",
      "[09/22/2022-22:03:42] [I] \n",
      "[09/22/2022-22:03:42] [I] === Device Information ===\n",
      "[09/22/2022-22:03:42] [I] Selected Device: NVIDIA Tegra X1\n",
      "[09/22/2022-22:03:42] [I] Compute Capability: 5.3\n",
      "[09/22/2022-22:03:42] [I] SMs: 1\n",
      "[09/22/2022-22:03:42] [I] Compute Clock Rate: 0.9216 GHz\n",
      "[09/22/2022-22:03:42] [I] Device Global Memory: 3956 MiB\n",
      "[09/22/2022-22:03:42] [I] Shared Memory per SM: 64 KiB\n",
      "[09/22/2022-22:03:42] [I] Memory Bus Width: 64 bits (ECC disabled)\n",
      "[09/22/2022-22:03:42] [I] Memory Clock Rate: 0.01275 GHz\n",
      "[09/22/2022-22:03:42] [I] \n",
      "[09/22/2022-22:03:42] [I] TensorRT version: 8.2.1\n",
      "[09/22/2022-22:03:43] [I] [TRT] [MemUsageChange] Init CUDA: CPU +229, GPU +0, now: CPU 248, GPU 2975 (MiB)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 248 MiB, GPU 2974 MiB\n",
      "[09/22/2022-22:03:44] [I] [TRT] [MemUsageSnapshot] End constructing builder kernel library: CPU 278 MiB, GPU 3004 MiB\n",
      "[09/22/2022-22:03:44] [I] Start parsing network model\n",
      "[09/22/2022-22:03:44] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-22:03:44] [I] [TRT] Input filename:   ./Flask/Models/efficientnet_b0-onnx.onnx\n",
      "[09/22/2022-22:03:44] [I] [TRT] ONNX IR version:  0.0.7\n",
      "[09/22/2022-22:03:44] [I] [TRT] Opset version:    9\n",
      "[09/22/2022-22:03:44] [I] [TRT] Producer name:    pytorch\n",
      "[09/22/2022-22:03:44] [I] [TRT] Producer version: 1.10\n",
      "[09/22/2022-22:03:44] [I] [TRT] Domain:           \n",
      "[09/22/2022-22:03:44] [I] [TRT] Model version:    0\n",
      "[09/22/2022-22:03:44] [I] [TRT] Doc string:       \n",
      "[09/22/2022-22:03:44] [I] [TRT] ----------------------------------------------------------------\n",
      "[09/22/2022-22:03:44] [W] [TRT] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[09/22/2022-22:03:44] [I] Finish parsing network model\n",
      "[09/22/2022-22:03:44] [W] Dynamic dimensions required for input: input__0, but no shapes were provided. Automatically overriding shape to: 1x3x224x224\n",
      "[09/22/2022-22:03:44] [I] FP32 and INT8 precisions have been specified - more performance might be enabled by additionally specifying --fp16 or --best\n",
      "[09/22/2022-22:03:44] [W] [TRT] Int8 support requested on hardware without native Int8 support, performance will be negatively affected.\n",
      "[09/22/2022-22:03:44] [W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.\n",
      "[09/22/2022-22:03:44] [I] [TRT] ---------- Layers Running on DLA ----------\n",
      "[09/22/2022-22:03:44] [I] [TRT] ---------- Layers Running on GPU ----------\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_0\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_1), Mul_2)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_3\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_4), Mul_5)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_6\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_7\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_8), Mul_9)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_10\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_11), Mul_12)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_13\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_14\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_15), Mul_16)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_17\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_18), Mul_19)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_20\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_21\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_22), Mul_23)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_24\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_25), Mul_26)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_27\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_28\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_29), Mul_30)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_31\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_32), Mul_33)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_34\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_35\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_36), Mul_37)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_38\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_39), Mul_40)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_41 + Add_42\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_43\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_44), Mul_45)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_46\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_47), Mul_48)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_49\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_50\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_51), Mul_52)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_53\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_54), Mul_55)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_56\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_57\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_58), Mul_59)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_60\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_61), Mul_62)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_63\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_64\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_65), Mul_66)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_67\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_68), Mul_69)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_70 + Add_71\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_72\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_73), Mul_74)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_75\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_76), Mul_77)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_78\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_79\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_80), Mul_81)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_82\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_83), Mul_84)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_85\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_86\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_87), Mul_88)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_89\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_90), Mul_91)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_92\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_93\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_94), Mul_95)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_96\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_97), Mul_98)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_99 + Add_100\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_101\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_102), Mul_103)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_104\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_105), Mul_106)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_107\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_108\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_109), Mul_110)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_111\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_112), Mul_113)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_114 + Add_115\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_116\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_117), Mul_118)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_119\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_120), Mul_121)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_122\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_123\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_124), Mul_125)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_126\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_127), Mul_128)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_129\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_130\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_131), Mul_132)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_133\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_134), Mul_135)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_136\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_137\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_138), Mul_139)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_140\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_141), Mul_142)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_143 + Add_144\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_145\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_146), Mul_147)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_148\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_149), Mul_150)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_151\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_152\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_153), Mul_154)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_155\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_156), Mul_157)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_158 + Add_159\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_160\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_161), Mul_162)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_163\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_164), Mul_165)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_166\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_167\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_168), Mul_169)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_170\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_171), Mul_172)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_173\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_174\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_175), Mul_176)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_177\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_178), Mul_179)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_180\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_181\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_182), Mul_183)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_184\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_185), Mul_186)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_187 + Add_188\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_189\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_190), Mul_191)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_192\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_193), Mul_194)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_195\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_196\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_197), Mul_198)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_199\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_200), Mul_201)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_202 + Add_203\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_204\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_205), Mul_206)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_207\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_208), Mul_209)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_210\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_211\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_212), Mul_213)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_214\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_215), Mul_216)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_217 + Add_218\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_219\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_220), Mul_221)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_222\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_223), Mul_224)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_225\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_226\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_227), Mul_228)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_229\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_230), Mul_231)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_232\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Conv_233\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] PWN(PWN(Sigmoid_234), Mul_235)\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] GlobalAveragePool_236\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] Gemm_238\n",
      "[09/22/2022-22:03:44] [I] [TRT] [GpuLayer] (Unnamed Layer* 253) [Shuffle]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09/22/2022-22:03:45] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +159, GPU +139, now: CPU 458, GPU 3186 (MiB)\n",
      "[09/22/2022-22:03:47] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +240, GPU +246, now: CPU 698, GPU 3432 (MiB)\n",
      "[09/22/2022-22:03:47] [I] [TRT] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
      "[09/22/2022-22:06:33] [I] [TRT] Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.\n",
      "[09/22/2022-22:11:15] [I] [TRT] Detected 1 inputs and 1 output network tensors.\n",
      "[09/22/2022-22:11:15] [I] [TRT] Total Host Persistent Memory: 132320\n",
      "[09/22/2022-22:11:15] [I] [TRT] Total Device Persistent Memory: 10992128\n",
      "[09/22/2022-22:11:15] [I] [TRT] Total Scratch Memory: 0\n",
      "[09/22/2022-22:11:15] [I] [TRT] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 9 MiB, GPU 51 MiB\n",
      "[09/22/2022-22:11:15] [I] [TRT] [BlockAssignment] Algorithm ShiftNTopDown took 90.4912ms to assign 5 blocks to 181 nodes requiring 10022913 bytes.\n",
      "[09/22/2022-22:11:15] [I] [TRT] Total Activation Memory: 10022913\n",
      "[09/22/2022-22:11:15] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 968, GPU 3686 (MiB)\n",
      "[09/22/2022-22:11:15] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 968, GPU 3686 (MiB)\n",
      "[09/22/2022-22:11:15] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +32, now: CPU 0, GPU 32 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 978, GPU 3707 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] Loaded engine size: 20 MiB\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 987, GPU 3709 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 987, GPU 3709 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +19, now: CPU 0, GPU 19 (MiB)\n",
      "[09/22/2022-22:11:16] [I] Engine built in 454.138 sec.\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +0, now: CPU 916, GPU 3710 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] Init cuDNN: CPU +0, GPU +0, now: CPU 916, GPU 3710 (MiB)\n",
      "[09/22/2022-22:11:16] [I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +20, now: CPU 0, GPU 39 (MiB)\n",
      "[09/22/2022-22:11:16] [I] Using random values for input input__0\n",
      "[09/22/2022-22:11:16] [I] Created input binding for input__0 with dimensions 1x3x224x224\n",
      "[09/22/2022-22:11:16] [I] Using random values for output output__0\n",
      "[09/22/2022-22:11:16] [I] Created output binding for output__0 with dimensions 1x1000\n",
      "[09/22/2022-22:11:16] [I] Starting inference\n",
      "[09/22/2022-22:11:19] [I] Warmup completed 8 queries over 200 ms\n",
      "[09/22/2022-22:11:19] [I] Timing trace has 113 queries over 3.04473 s\n",
      "[09/22/2022-22:11:19] [I] \n",
      "[09/22/2022-22:11:19] [I] === Trace details ===\n",
      "[09/22/2022-22:11:19] [I] Trace averages of 10 runs:\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 27.0513 ms - Host latency: 27.111 ms (end to end 27.1208 ms, enqueue 8.40693 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 27.1056 ms - Host latency: 27.1658 ms (end to end 27.1756 ms, enqueue 9.56684 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.6538 ms - Host latency: 26.7125 ms (end to end 26.7222 ms, enqueue 8.17506 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.7256 ms - Host latency: 26.7851 ms (end to end 26.7948 ms, enqueue 8.78683 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 27.0019 ms - Host latency: 27.061 ms (end to end 27.0811 ms, enqueue 9.66404 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.6577 ms - Host latency: 26.7162 ms (end to end 26.726 ms, enqueue 9.13989 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 27.0735 ms - Host latency: 27.1329 ms (end to end 27.1428 ms, enqueue 9.43053 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.677 ms - Host latency: 26.7358 ms (end to end 26.7456 ms, enqueue 8.34355 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.6624 ms - Host latency: 26.7213 ms (end to end 26.7311 ms, enqueue 8.99082 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 27.0742 ms - Host latency: 27.1346 ms (end to end 27.1445 ms, enqueue 9.62764 ms)\n",
      "[09/22/2022-22:11:19] [I] Average on 10 runs - GPU latency: 26.9839 ms - Host latency: 27.0433 ms (end to end 27.0528 ms, enqueue 8.35449 ms)\n",
      "[09/22/2022-22:11:19] [I] \n",
      "[09/22/2022-22:11:19] [I] === Performance summary ===\n",
      "[09/22/2022-22:11:19] [I] Throughput: 37.1133 qps\n",
      "[09/22/2022-22:11:19] [I] Latency: min = 26.6396 ms, max = 30.4232 ms, mean = 26.9333 ms, median = 26.7449 ms, percentile(99%) = 30.4158 ms\n",
      "[09/22/2022-22:11:19] [I] End-to-End Host Latency: min = 26.6497 ms, max = 30.4328 ms, mean = 26.944 ms, median = 26.7561 ms, percentile(99%) = 30.4258 ms\n",
      "[09/22/2022-22:11:19] [I] Enqueue Time: min = 7.65991 ms, max = 12.3303 ms, mean = 8.93343 ms, median = 8.72314 ms, percentile(99%) = 12.2024 ms\n",
      "[09/22/2022-22:11:19] [I] H2D Latency: min = 0.0549316 ms, max = 0.0622559 ms, mean = 0.0564084 ms, median = 0.0560303 ms, percentile(99%) = 0.0610352 ms\n",
      "[09/22/2022-22:11:19] [I] GPU Compute Time: min = 26.5808 ms, max = 30.3638 ms, mean = 26.874 ms, median = 26.6865 ms, percentile(99%) = 30.3557 ms\n",
      "[09/22/2022-22:11:19] [I] D2H Latency: min = 0.00170898 ms, max = 0.00341797 ms, mean = 0.00289512 ms, median = 0.00292969 ms, percentile(99%) = 0.00341797 ms\n",
      "[09/22/2022-22:11:19] [I] Total Host Walltime: 3.04473 s\n",
      "[09/22/2022-22:11:19] [I] Total GPU Compute Time: 3.03676 s\n",
      "[09/22/2022-22:11:19] [I] Explanations of the performance metrics are printed in the verbose logs.\n",
      "[09/22/2022-22:11:19] [I] \n",
      "&&&& PASSED TensorRT.trtexec [TensorRT v8201] # /usr/src/tensorrt/bin/trtexec --onnx=./Flask/Models/efficientnet_b0-onnx.onnx --explicitBatch --int8 --saveEngine=./Flask/Models/efficientnet_b0-trt_int8.plan\n",
      "./Flask/Models/efficientnet_b0-trt_int8.plan\n",
      "./Triton/Models/efficientnet_b0-trt_int8/config.pbtxt\n",
      "./Triton/Models/efficientnet_b0-trt_int8/1/model.plan\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'efficientnet_b0-onnx'\n",
    "trt_model = 'efficientnet_b0-trt'\n",
    "precision = 'int8'\n",
    "triton_config = \"\"\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "save_tensorrt_model(onnx_model, trt_model, precision, triton_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
